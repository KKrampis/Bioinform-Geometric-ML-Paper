We start this review by introducing the concept of symmetry, which is central in physical, mathematical and biological systems, in addition to other areas of human endeavor such as social structures. It has been established since the early 20th century that fundamental laws of nature originate in symmetry, for example through the work of Noether and Wigner [ref,ref].In the last decade technologies such as genomic sequencing, have enabled an exponential increase [ref] of the data available that describe the fundamental elements, structure and function of biological systems. Other endeavors from fields as diverse as physics [ref collider], and the rise of data generation and capture through social media platforms [ref], have generated even larger datasets. This data explosion, has also been fundamental for the ever accelerating advancements in the field of machine learning, deep learning and artificial intelligence, where we now  have algorithms that can be trained to make discoveries from the data at a level that matches closely human intuition.

As in any field (including bioinformatics) that develops rapidly whithin the span of a few years,deep learning and artificial intelligence researchers have developed hundreds of successful algorithms, however with a few unifying principles (details in Bronstein et al., [ref]). In this seminal `proto-book` by Bronstein et al., a range of systematization principles for the different artificial neural network architectures from the deep learning field was presented, based on the concepts of symmetry that is formalized within the mathematical field of group theory. The authors introduced the concept of Geometric Deep Learning, and demonstrated how the group theoretic principles of iso- and auto-morphism, along with invariance and equivariance of functions can be used in composing and describing various deep learning algorithms. 

Before proceeding towards introducing these concepts further, we will describe deep learning and artificial neural networks that are the center of artificial intelligence,through a simple mathematical description. Assume a dataset consisting of n pairs of {x_i,y_i}_i^n ,with the x_i being the data points and y_i the labels. Each data point can be anything ranging from a set of numbers, vectors, images (that can be decomposed to vectors), graphs structures with nodes and edges, molecules (that can be represented as graphs) etc.Whether the data points originate from scientific experiments,clinical researh, databases or other records, a x_i molecule for example represented as a graph, could have a binary label of y_i=1 inhibits cancer growth or y_i=0, does not inhibit cancer. The labels can also be continuous numbers (i.e. y_i=0.3 means 30% inhibition, an also each y_i can be a set (i.e. y_i = {0,1,0} means does not inhibit,has toxicity,is not metabolized, for each zero and one respectively). 