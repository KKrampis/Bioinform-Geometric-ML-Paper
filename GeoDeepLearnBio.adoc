= Mathematical Principles of Graph Neural Networks and Geometric Deep Learning
for Bioinformatics
 Konstantinos Krampis
:stem:

== ABSTRACT
 Following the exponential growth of the machine and deep learning
field in recent years, artificial neural network models have found significant
applications towards bioinformatics data analysis. The various omics datasets
are commonly represented in a graph format, such as for example protein or gene
interaction networks, molecular structures and cellular signalling pathways.
Unlike structured images, video and text that are a staple for training
artificial neural networks, graph data are in the non-euclidean domain and
require significantly different algorithmic approaches. The machine learning
research community, has developed a range of new algorithms for training
artificial neural networks with graph data. These novel approaches and their
importance for the bioinformatics field is established herein, through
exhibition of the undelying mathematical foundations from group theory,
functional analysis and linear algebra. Furthermore, it is argued that the most
recent developments in the field such as geometric deep learning on data
manifolds, can also significantly accelerate scientific discovery in
bioinformatics by enabling new approaches to understand complex datasets.
Finally, we conclude this opinion article with the options for implementations
through the available software frameworks, as guideline for transitioning from
the mathematical principles to practical graph machine learning tools for
bioinformatics applications.


== INTRODUCTION

Symmetry and invariance is a central concept in physical, mathematical and
biological systems, in addition to other areas of human endeavor such as social
structures.. It has been established since the early 20th century that
fundamental laws of nature originate in symmetry, for example through the work
of Noether and Wigner [REF,REF].In the last decade, technologies such as genomic
sequencing have enabled an exponential increase [REF] of the data that describe
the fundamental elements, structure and function of biological systems. Other
endeavors from fields as diverse as physics [REF collider], and the rise of
social media platforms [ref], have resulted in datasets of scale not previously
available to scientists. This data explosion, has also been fundamental for the
ever accelerating advancements in the field of machine learning, deep learning
and artificial intelligence, where we now  have algorithms that can be trained
to make discoveries from the data at a level that matches closely human
intuition.

As in any field (including bioinformatics) that develops rapidly whithin the
span of a few years,deep learning and artificial intelligence researchers have
developed hundreds of successful algorithms, however with a few unifying
principles (Bronstein et al., [ref]). In the seminal `proto-book` by Bronstein
et al., a range of systematization principles for the different artificial
neural network architectures from the deep learning field was presented, based
on the concepts of symmetry that is formalized within the mathematical field of
group theory. The authors also introduced the concept of Geometric Deep
Learning, and demonstrated how the group theoretic principles of iso- and
auto-morphism, along with invariance and equivariance of functions, can be used
in composing and describing various deep learning algorithms. 

== THE STRUCTURE AND FUNCTION OF ARTIFICIAL INTELLIGENCE

Before proceeding towards introducing these concepts further, we will describe
the structure and function of deep learning and artificial neural networks that
are the center of artificial intelligence,through a simple mathematical
description. Assume a dataset consisting of _n_ pairs of stem:[{x_i,y_i}_i^n]
,with the stem:[x_i] being the data points and stem:[y_i] their labels. Each
data point can be for example be a set of numbers, vectors, images (that can be
decomposed to vectors), graphs structures with nodes and edges, and molecules
(that can be represented with a graph structure). Whether the data points
originate from scientific experiments,clinical researh, databases or other
records, a stem:[x_i] molecule for example represented as a graph, could have a
binary label of stem:[y_i=1] "inhibits cancer growth", or stem:[y_i=0] "does not
inhibit cancer". The labels can also be continuous numbers such as for example
stem:[y_i=0.3] meaning 30% inhibition, while also each stem:[y_i] could be a set
such as stem:[y_i = {0,1,0}] corresponding does not inhibit,has toxicity,is not
metabolized, for each zero and one respectively.

Similar to a brain neural network,the computational abstractions used to model
neurons in machine learning and artificial intelligence,represent them as signal
aggregators and thresholding units. Specifically, each artificial neuron
performs summation of incoming signals from connected neurons, and "fires" when
the aggregated signals are above a certain threshold. This can be described with
the summation stem:[sigma_(kl)=sum_(k=1)^n w_(kl)**x_(kl) + b]. By following the
biological analogy the stem:[w_(kl)] is the connection weight (similar to the
brain learning through strengthening connections among neurons, REF)  with
dendrite _l_ from neuron _k_, and similarly stem:[x_(kl)], while _b_ is the
threshold that needs to be met with stem:[w_(kl)**x_(kl)>0] in order to the neuron
to activate.In the simplest version of "fully connected" artificial neural
networks, each neuron _k_ receives a copy of the input _i_, resulting im _k*i_
incoming and outgoing connections per neuron.  The activation threshold also
affects the "logit" or otherwise activation function stem:[varphi(sigma_(kl))],
which is usually non-linear (REF), and its output value is transmitted to its
connected neurons through the outgoing "axons" (using the biological
analogy,REF).Multiple layers of such computational models of neurons connected
together, along with multiple connections per layer each having each own weight
stem:[w_(kl)], forms an Artificial Neural Network (ANN).

Mathematically, a trained ANN is an estimator of the function
stem:[y_(pred_i)=mathcal "F"(x_i)], correlating output labels stem:[y_i] to the
input stem:[x_i].Furthermore, the function stem:[mathcal "F"] is a composition
of the stem:[varphi(sigma_(kl))] functions of the all the neurons interconnected in
the artificial network graph (REF).This is the most powerful aspect of the ANNs,
since the accumulated non-linearities of each stem:[varphi(sigma_(kl))], enable the
network to learn on multi-dimensional input data with complex distributions, and
achieve higher clasification power compared to traditional statistical methods
(REF). With hundreds of papers and monographs that have been written on the
technical details of training ANNs,and since the focus of the present review
manuscript are networks designed with graph and other non-euclidean datasets, we
will attempt to summarize the training in few sentences and refer the reader to
REF for further details. 

Essentially, ANN training involves estimating the overall function
stem:[y_(pred_i)=mathcal "F"(x_i)] by estimating the set of weights w_kl for
minimizing an error function mathcal "E", which quantifies the difference
between the predicted stem:[y_(pred_i)] and actual labels stem:[y_i]. The weight
estimation by the algorithms (explained below) takes place by fitting the
network on a large training dataset of stem:[{x_i,y_i}_i^n], followed by
evaluation of the training using smaller testing and validation data subsets. By
identifying a set of weights which best fit the data model, the ANN settles on
the optimal parameters that estimate the stem:[varphi(sigma_(kl))] function of each
interconnected neuron. Consequently, the overall stem:[mathcal "F"(x_i)] is also
estimated,since it is the composition of the individual stem:[varphi(sigma_(kl))] 
functions, through the overall connectivity of the network. Following an initial
estimated guess for initilizing the weights,the algorithms "feed-forward" the
stem:[x_i] training data to the network (REF), and when the complete dataset has
been processed by the network the mathcal "E"error is calculated based on the
difference between stem:[y_(pred_i)] and stem:[y_i]. Furthermore, multiple
training cycles take place ("epochs") during each the complete dataset is
processed by the network, and at the end of each cycle "backpropagation" is
performed. The backpropagation involves an optimization algorithm process..   

== ARTIFICIAL INTELLIGENCE AND SYMMETRY

Returning to Geometric Deep Learning, and how the group theoretic principles 


