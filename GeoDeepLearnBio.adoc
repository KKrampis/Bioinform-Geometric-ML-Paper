= Mathematical Principles of Artificial Neural Networks and Machine Learning for Bioinformatics

Konstantinos Krampis*^1^, Eric Ross^2^, Olorunseun O. Ogunwobi^1^, Grace Ma,^3^ Raja Mazumder,^4^ Claudia Wultsch^1^


:stem:
:bibtex-file: ./GDL-bibliography/references.bib

^1^Belfer Research Facility, Biological Sciences, Hunter College, City University of New York
^2^Fox Chase Cancel Center 
^3^Temple University
^4^George Washington University

^*^Corresponding Author, _agbiotec@gmail.com_

== ABSTRACT 
Following the exponential growth of deep learning methodology
within the machine field in recent years, the use of Artificial Neural Network
(ANNs) models has also had significant applications towards bioinformatics data
analysis. Various types of datasets have been used for machine learning by
training ANNs for inference and classification, including for example protein
or gene interaction networks, molecular structures and cellular signalling
pathways. However, unlike images, video and text that are commonly used in the
computer science and engineering fields and which contain regular data
structures, bioinformatics datasets present unique challenges and require
different algorithmic approaches.  The recent development of the geometric deep
learning approach within the machine learning field, is promising towards
accelerating scientific discovery also in bioinformatics by enabling new
approaches to understand complex datasets.  This novel methodology and its
importance for bioinformatics machine learning is demonstrated herein, through
presentation of the undelying mathematical foundations from group theory, graph
data structures and linear algebra.  Furthermore, the structure and functions
of ANNs algorithm that form the core principles if artificial intelligence are
explained, in relation to the bioinformatics data domain.  Overall, the
manuscript provides guidance for researchers to understand the mathematical
principles required for practicing machine learning and artificial
intelligence, in addition to the special considerations towards bioinformatics
applications.


== INTRODUCTION

Symmetry and invariance is a central concept in physics, mathematical and
biological systems, and has been established since the early 20th century that
fundamental physics and chemistry principles are based on symmetry
cite:[noether1918invariante].  In the last decade, technologies such as genomic
sequencing have enabled an exponential increase cite:[katz2022sequence] of the
data that describe the molecular elements, structure and function of biological
systems. Furthermore, data generation in fields as diverse as physics, software
development and social media cite:[clissa2022survey], have resulted in datasets
of scale not previously available to scientists. This data explosion, has been
fundamental for the ever accelerating advancements in the field of machine
learning, deep learning and artificial intelligence, where we now  have
algorithms that can be trained to make discoveries from the data at a level
that matches closely human intuition.

As in any field, including bioinformatics, that develops rapidly whithin the
span of a few years, deep learning and artificial intelligence researchers have
developed hundreds of successful algorithms, however with a few unifying
principles . In a seminal `proto-book` by Bronstein et al.
cite:[bronstein2021geometric], a range of systematization principles for the
different artificial neural network architectures from the deep learning field
was presented, based on the concepts of symmetry that is formalized within the
mathematical field of group theory. The authors also introduced the concept of
geometric deep learning, and demonstrated how the group theory, function invariance 
and equivariance principles, can be used as basis towards composing and describing 
the various deep learning algorithms. Using these principles as basis, in the
present manuscript we review the structure of ANNs, and the function of machine
learning algorithms with bioinformatics data, alogin with a review of mathematical
structures that support development of artificial intelligence applications in 
this field.

== THE STRUCTURE OF ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS

We will first describe the structures and function of deep learning and
Artificial Neural Networks (ANNs) that are the foundation of artificial
intelligence cite:[li2019deep]. Assume a dataset consisting of _n_ pairs of
stem:[(x_i,y_i)_n], with the stem:[x_i] being _n_ data points and stem:[y_i]
their labels. Each stem:[x_i] data point can be for example be a number, a
vector (array of numbers), a matrix (grid of numbers) storing bioinformatics
data.  The data and labels can be of various formats, such as binary
(two-option) as for example stem:[y_i=1] "inhibits cancer growth", or
stem:[y_i=0] "does not inhibit cancer". The labels can also be continuous
numbers such as for example stem:[y_i=0.3] meaning 30% inhibition, while also
each stem:[y_i] could be a composite label such as stem:[y_i = (0,1,0)]
representing respectively drug attributes such as '0 - no inhibition', '1 - yes
for toxicity', '0 - not metabolized'. Similarly the data points can also be
composite such as for example stem:[x_i = (50,100)] representing two measuments
for a single biological entity. Independently of the label structure, the deep
learning algorithms and overall the goal of artificial intelligence for bioinformatics,
is to perform classification by predicting labels for newly generated data,
following pre-training with existing data for which the labels are known. 

The simplest structure of an artificial neural network as shown on *Fig.1* is
"fully connected", with each neuron _k_ in the ANN having a number of incoming
and outgoing connections corresponding to the number of neurons in previous and
next layer in the neural network. For example the neuron stem:[k_0^(1)] of the
_First Layer 1_ on *Fig.1*, has stem:[n = 2] incoming and stem:[n = 3] outgoing
connections, corresponding respectively to the "input layer" with two neurons,
and three connections with the neurons of the internal - "hidden layer" named
_Second Layer 2_ on the same figure. The internal layers are called "hidden"
since they do not receive input data directly, similarly to the neurons performing
cognition in animal brains as opposed to sensory neurons. While the hidden
layers can have an arbitrary number of neurons each, based on the complexity of
the label classification problem we need the ANN to resolve cite:[uzair2020effects], the input
layer has the numbers of neurons corresponding to the input data structure. On
*Fig. 1* for example we have two input neurons, and the data can be of the
form stem:[x_i = (50,100)]. Finally, the output layer has a number of neurons 
corresponding to the number of labels stem:[y_i] in the training data, per input 
data point, and in the example of *Fig. 1* there would be a single label   

.The signal aggregation taking place on the second neuron stem:[sigma_(k_1^((2)))] of the second hidden layer, can be expressed with the formula stem:[sigma_(k_1^((2)))=sum_(k_(0,1,2))^((1)) w_(k0)**x_(k0) + w_(k1)**x_(k1) + w_(k2)**x_(k2) - b], which is the aggregation of neuron signals from the first layer, shown as red arrows on the figure. The _b_ is the threshold that needs to be overcome by the aggregation in order for the neuron to fire, and following the neuron will transmit a signal along the line shown towards the output neuron on the final layer on the figure. The reader should refer to the text for more details.
[#img-fig1] 
image::graphviz.svg[Fig1]

Similar to a brain neural network, the computational abstractions used in
machine learning and artificial intelligence, model neurons as computational
units performing signal summation and thresholding.  Specifically, each
artificial neuron performs a summation of incoming signals from its connected
neighbooring neurons from the previous layer (left side of the neuron) on the
network (*Fig.1*), when the aggregated signals reach a certain threshold, and
then it "fires" by transmitting a singal to the next layer.  The signals can be
either data directly from the input layer, or values generated through
computation in the neurons of the intermediate layers. The summation and
thresholding computation within each neuron is represented with the summation
function stem:[sigma_(k)=sum_1^k w_(k)**x_(k) - b] , where the stem:[w_(k)]
represents the connection weight of neighboring neuron _k_ (*Fig.1*). The next
term stem:[x_(k)] multiplied by the weight strength, is the incoming signal
from the neigbooring neuron for which the weighted connection is established.

The weight value is the only variable element in the ANNs that is adjusted by
the algorithms during training with the input data, similarly to the biological
brain where learning takes place by strengthening connections among neurons.
Unlike the biological brain, the ANNs used in practice for data analysis have
fixed connections between the neurons and structure of the neural network does
not change during training. The last term _b_ in the summation, represents a
threshold that needs to be surpassed as stem:[sum_1^k w_(k)**x_(k) > b], in
order for the neuron to activate and "fire" a signal.  Finally, the output
value of the neuron, is determined by applying to the summation value a "logit"
function stem:[varphi(sigma_(k))] which is non-linear cite:[li2019deep].   This
final value is the output of the firing neuron, and  is transmitted to its
connected neurons in the next layer through the outgoing connections (or
"axons" in the biological analogy).  Multiple layers of such computational
models of neurons connected together in layers (*Fig.1*), along with multiple
connections per layer each having each own weight stem:[w_(k)], forms the
Artificial Neural Network (ANN).

Mathematically, a trained ANN is a function stem:[f] that predicts the labels
stem:[y_(pred_i)] for the input data stem:[x_i], such as for example 'no
inhibition', 'yes for toxicity' etc. for data representing drug
molecules.  The function stem:[f] is non-linear and is estimated by fitting a
training dataset, which correlates labels stem:[y_i] to data points stem:[x_i].
Most important, stem:[f] is a composition of the stem:[varphi(sigma_(k))]
thresholding functions of the each neuron interconnected in the artificial
network graph cite:[li2019deep]. This is key aspect of the ANNs, since the
non-linearities present in each individual logit neuron function
stem:[sigma_(k)], are aggregated across the neural network and on the function
stem:[f], which enables fitting complex multi-dimensional input data with
non-linear distributions. This is the key fact that enables ANNs to achieve
higher clasification power compared to traditional statistical methods
cite:[tang2019recent]. With hundreds of papers and monographs that have been
written on the technical details of training ANNs,and since the focus of the
present review manuscript are networks designed with graph and other
non-euclidean datasets, we will next attempt to summarize the training in few
sentences and refer the reader to the citations for further details. 

In summary, the ANN algorithm is using the training data to identify a function
stem:[f] that predicts labels from the data such as stem:[y_(pred_i)=f(x_i)].
As mentioned previously, stem:[f] is a composition of the
stem:[varphi(sigma_((k_0)^1))] functions of the each neuron in the ANN, and as such
the training is the estimation of the weights stem:[w_k] of the neuron
connections, while minimizing the error stem:[E] based on the difference
between stem:[y_(pred_i)] and stem:[y_i].  This error quantifies the neural
network precision by comparing predicted stem:[y_(pred_i)] and actual labels
stem:[y_i]. The neuron connection weight stem:[w_k] estimation by the algorithm
takes place through fitting the network function stem:[f] on a large training
dataset of stem:[{x_i,y_i}_i^n], pairs of input data and labels, while it
evaluates the error stem:[E] using smaller testing and validation data subsets.
The training algorithm works by making an initial estimated guess for
initializing the weights, and then performing multiple cycles (called "epochs")
of fitting stem:[x_i] the training data to the network. At the end of each
cycle "backpropagation" is performed cite:[tang2019recent], which involves
gradient descent optimization, in order to fine tune the weights of the
individual neurons in stem:[sigma_(k)=sum_(k=1)^n w_(k)**x_(k) + b].  The
gradient descent (REF) searches the possible combinations of weight values, and
since it is a heuristic algorithm it minimizes but cannot reach zero error
stem:[E]. At the completion of multiple training cycles the training algorithm
identifies a set of weights which best fit the data model, and the ANN settles
on the optimal parameters that estimate the stem:[varphi(sigma_(k))] function
for each interconnected neuron.  Consequently, the overall stem:[f(x_i)] is
also estimated,since it is the composition of the individual
stem:[varphi(sigma_(k))] neuron functions.  Once the artificial neural network
training has ben completed by finding the most optimal set of weights, it is
now ready to be used for label prediction with new, unknown stem:[x_i] data.

== ARTIFICIAL INTELLIGENCE, GROUP THEORY, SYMMETRY AND INVARIANCE

We conclude, by briefly reviewing how the principles of group theory,
symmetry and invariance, are a foundational framework to understand the
function of machine learning algorithms, and examine the classifying power of
ANNs in relation to statistical variance and non-homogeneity in the data. In
summary, symmetry is the study of space and structure, with examples
referring to to geometric and algebraic constructs in mathematics, material
elements in physics and molecular biology structures. Invariance of an object
under transformation, is the property of changing the position of the object
in space, such as shifting a drug molecule or rotating a cancer histology
image, while leaving the properties of the object unchanged
cite:[bronstein2021geometric]. In these examples, the drug remains potent
following rotation of the molecule, and the tissue is still recognized as
cancerous based on the histology image. 

Following the terminology of Bronstein et al., we consider the input
stem:[x_i] from a data domain stem:[Omega], which has a specific structure
corresponding to the data type used for training the ANN. For example,
microscopy images are essentially 2-dimensional numerical grids (matrices) of
_n x n_ pixels, with each pixel having a value for light intensity.  In this
case the data domain is composed of integers (stem:[ZZ]) as grid stem:[Omega:
ZZ_n xx ZZ_n], which can have all possible combinations of pixel intensities.
Similarly, for color images the data domain is stem:[x_i:Omega to ZZ_n^3 xx
ZZ_n^3], with 3 integer grids each representing the green, blue and red
layers composing the color image. The ANN data fitting and label prediction
function stem:[y_(pred_i)=f(x_i)] is applied on a "signal" stem:["X"(Omega)]
from the domain, which is a subset of the domain stem:[Omega] with the
specific images used for training the neural network. 

[.left]
[graphviz, target=Fig2a, format=svg]
....
digraph directedgraph {
  node [shape=circle, style=filled, color=lightblue, fontname=Arial, fontsize=12];
  edge [color=gray, penwidth=1.5];

  A [label="Node A", color=green];
  B [label="Node B", color=blue];
  C [label="Node C", color=red];
  D [label="Node D", color=yellow];
  E [label="Node E", color=orange];
  F [label="Node F", color=purple];

  A -> B;
  A -> C;
  B -> C;
  B -> D;
  C -> D;
  C -> E;
  D -> E;
  D -> F;
}
....


[.left]
[graphviz, target=Fig2b, format=svg]
....
digraph grid_layout {
  node [shape=circle, style=filled, color=lightblue, fontname=Arial, fontsize=12, width=0.6, height=0.6];
  edge [color=gray, penwidth=1.5];

  A [label="Node A", color=green];
  B [label="Node B", color=blue];
  C [label="Node C", color=red];
  D [label="Node D", color=yellow];
  E [label="Node E", color=orange];
  F [label="Node F", color=purple];

  {rank=same; A; B; C;}
  {rank=same; D; E; F;}

  A -> B -> C;
  D -> E -> F;
  A -> D;
  B -> E;
  C -> F;
}
....

Another important data structure for bioinformatics is a _graph_
latexmath:[$G = (V, E)$] that is composed of _nodes_ latexmath:[$V$]
representing biological entities, and _edges_  which are connections between
pairs of nodes (*Fig.2*).  In a specific instance of a graph, the present
edges are a subset of all possible edges between nodes.An example graph data
structure for a biological molecule such a protein or a drug, would represent
respectively the amino acids or atoms as node entities, and the chemical
bonds between each of these entities as edges. Noted that the nodes and their
edge connections are simply an abstraction of the real-world object, and the
edges can correspond to either the carbonyl-amino (C-N) peptide bonds between
amino acids, molecular interactions across the peptide chain leading to three
dimensional protein structure, or the bonds in the chemical structure of a
small molecule in the preceding examples. Next, attributes in the source data
such as for example polarity and amino acid weight , or drug binding
properties of the chemical molecule can be represented as node attributes
latexmath:[$s$]-dimensional , where _s_ are the number of attributes in the
data for each object represented as a node. Similarly the edges or entire
graphs, can have attributes, for data on the molecular interactions
represented by the edges or the whole molecular entity (ex.  protein or
drug).

From a mathematical and algorithmic perspective, images are a special case of
graphs where the nodes have a set of connection with edges in a structured
pattern that form of a grid. Under this perspective, the graph nodes are the
pixels of the images, and the edges the connections specifying the adjacency
of the pixels (*Fig.2*).  With this realization in place, we can now
examine the analytical and classification power of ANNs given variance in the
data, for both data types in cases where we have in the dataset shifted or
rotated input images or molecules represented as graphs.  We establish this
through the principles of group theory, symmetry and invariance. These are
the foundational mathematical and algorithmic principles that model the
performance and output of machine learning algorithms ANNs in relation to the
variability in the dataset. Consecutively, these principles can then be
extrapolated for other types of data beyond graphs and images, 
for which ANNs are trained for prediction and classification.

A _symmetry group_ latexmath:[$G$] can be defined between the the input dataset
used for training the ANN, which is a subset "signal" stem:["X"(Omega)] of all
possible images and graphs from a data domain stem:[Omega] that can be formed
for example on a  grid (*Fig.2*). Therefore, a symmetry latexmath:[$g$]
otherwise called a group action, is a transformation that preserves the
properties of the data (for example the objects in the image and edge
connections on the graph), and the set latexmath:[$G$] of all possible
transformations is the symmetry group. The members of the symmetry group
latexmath:[$g \in G$] are the associations of two or more points on the grid
latexmath:[$u,v\in \Omega$] between which an image or graph can be rotated,
shifted etc. without distortion of the data.  Therefore, The key aspect of the
formal mathematical definition of the group, is that the data attributes are
preserved so that an image for example is not distorted when moved on the
plane, or similarly not changing the connnections between the graph edges and
nodes representing molecule elements, when the molecule is rotated. something
that is common in noisy, real-world data. Essentially, the symmetry group
represented as latexmath:[$G$] ensures through these associations of points on
the plane for the present example, that the data integrity is preserved for a
set of changes that belong within the symmetry group. 

This is an important aspect of modeling data classification and training of
ANNs through group theory and symmetry, so we can formalize the resilience of
machine learning algorithms and their perfomance, in relation to the
variability in the data.  Here we presented the group notion with a more
data-centric definition which nonetheless follows the mathematical formalism,
where we do not specify what the group operations but only how they can
transform the input data. Therefore, different types of data can have the same
symmetry group, where transformations of different types of data are performed
by the same group operation. For example, an image with a triangle which
essentially is a graph with three nodes, can have the same rotational symmetry
group as a graph of three nodes or a numerical sequence of three elements.

As with real chemical or biological molecules measured in an experiment, graphs
which represent them as described earlier have the property that the nodes in
set latexmath:[$V$] are usually provided in any order. This does not change the
meaning of the data, and as long as the edges **E** representing the
connections between the molecules stay the same we have the same molecular
entity indepentently of the ordering of **V**, and is this case two graphs for
the same molecule are _isomorphic_. Furthermore, any machine learning
algorithms performing operations on graphs, should not depend on the ordering
of nodes so that classification and pattern recognition with ANNs and
artificial intelligence is not affected by shifts and rotations in real-world.
This is something that is taken for granted with human intelligence, for
example where for example we can recognize an object even when a photograph is
rotated or at an angle. Returning to our formal definitions, in order for ANNs
algorithms to equivalently recognize _isomorphic_ graphs the functions
stem:[varphi(sigma_(k))]acting on graphs should be _permutation invariant,
meaning that for any two  graphs the outcomes of these functions are identical
independently of the ordering of the nodes **V**. This concept can be exactly
applied to images, which as mentined previously are special cases of fully
connected graphs and similarly for other data types.

Since both examples of the image and graphs are similarly points on a grids on
a two dimemensional plane, we can use linear algebra and specifically a matrix,
to represent the data transformations. Furthermore, the use of matrices enables
us to connect the group symmetries with the actual data, through assigning
matrix multiplications that represent the data transformations through
modification of the coordinates of the objext on the plane as a result of the
multiplication (*Fig. 2b*). The dimensions of the matrix latexmath:[$n \times
n$] is usually similar to these of the signal space stem:["X"(Omega)] for the
data (for example, stem:[nxn] images), and does not depend on the size of the
group i.e. the number of possible symmetries, or the dimensionality of
underlying data domain latexmath:[$\Omega$]. With this definition in place, the
symmetries between modified data objects are a result of of _linear_ group
actions - transformations. 

Having used matrix and linear transformations as basis for formalizing
variability in the data, we will now conclude by establishing also the
mathematical framework for resilience of the ANNs algorithm pattern recognition
in relation to deformation in the data.  While our framework is on a
two-dimensional, grid data domain latexmath:[$\Omega$], formalisms developed
here can be extrapolated without loss of generality to any number of dimensions
or data formats. We will first connect matrices to group actions
latexmath:[$g$] (rotations, shifts etc.) in the symmetry group latexmath:[$g
\in G$], by defining a function latexmath:[$\theta] that maps the group to a matrix as
latexmath:[$\theta : G \rightarrow \mathbf{M}]. As mentioned previously and
demonstrated on *(Fig. 2b)* this is a matrix  latexmath:[$\mathbf{M} in \R^{n
\times n}$] of numerical values (integers, fractions, positive and negative),
which when multiplied to the coordinates values of the object on the plane
latexmath:[$\Omega$], it rotates or shifts the object coordinates for the exact
amount correponsing to the group action within the symmetry group.

With these definitions in place, we can establish the resilience and
performance of the ANNs with noisy, real-world data, as estimators of the
overall function stem:[y_(pred_i)=f(x_i)] that fits the training data in order
to recognise future patterns with new, unknown data. We define that the
estimator function of the ANN to be _invariant_ if the condition for the input
data holds such as latexmath:[$f(\mathbf(M) x_i) = f(x_i)$] for all matrices
representing actions latexmath:[$\fg \in \fG$] within the symmetry group. The
condition required therefore for the function to be invariant, is for the
function output value to be equal with both the original input stem:[x_i] and
the one multiplied by the transformation matrix latexmath:[$f(\mathbf(M) x_i]
representing the group action. Therefore, the output values
stem:[y_(pred_i)=f(x_i)] by the ANN - artificial neural networks which are
essentially predicted output labels (i.e stem:[y_i] = potent drug / not potent
etc.) based on the input data, are resilient to noisy and deformed real-world
data.  In other cases, the estimator function approximated by the ANN can be
equivariant defined as latexmath:[$f(\mathbf(M) x_i) = \mathbf(M)f(x_i)$],
which means that the label prediction result of the ANN is shifted equally to
the shift in the input data.

Up to this point, we have discussed only discrete tranformations in linear
algebra terms, with matrix multiplications that result in a shift of
coordinates and rigid transformations of the data, such as a rotation of the
image or the graph by a specific angle on the grid stem:[Omega]. However, we
can have also also have continuous, more fine grained shifts which is rather
common that the exception with real-world data. The ANNs algorithms should be
able to recognize patterns, classify and label the data without any loss of
performance also in this case, and mathematically continuous transformation
follow equally with the invariant and equivariant functions described earlier.
If for example the domain latexmath:[$\Omega$] contains data that have smooth
transformations and shifts, such as for example moving images (video) or shifts
of molecules and graphs that preserve _continuity_ in a topological definition
[REF] in this case we have _homeomorphisms_ between members of the symmetry
group. Furthermore, if the rate if continuous transformation of the data is
quantifiable, meaning that the function latexmath:[$\theta] that maps the group
to a matrix is _differentiable_, then the members of the symmetry groups will
be part of a _diffeomorphism_. As it follows from the principles of calculus,
in this case infinitely multiple matrices latexmath:[$f(\mathbf(M)] will be
generated accordingly by latexmath:[$\theta] for the continuous change of the
data coordinates at every point. These differentiable data structures are
common with manifolds, which for example could be used to represent proteins in
fine detail, as a molecule cloud with all atomic forces around the structure,
instead of just the discrete, abstract representation of nodes and edges of a
graph. Finally, if the manifold structure includes includes also a metric of
_distance_ between its points to further quantify the data transformations, in
this case we will have an _isometry_.

bibliography::[]

