= Mathematical Principles of Artificial Neural Networks and Machine Learning for Bioinformatics

Konstantinos Krampis*^1^, Eric Ross^2^, Olorunseun O. Ogunwobi^1^, Grace Ma,^3^ Raja Mazumder,^4^ Claudia Wultsch^1^


:stem:
:bibtex-file: ./GDL-bibliography/references.bib

^1^Belfer Research Facility, Biological Sciences, Hunter College, City University of New York
^2^Fox Chase Cancel Center 
^3^Temple University
^4^George Washington University

^*^Corresponding Author, _agbiotec@gmail.com_

== ABSTRACT 
Following the exponential growth of the machine and deep learning field in
recent years, Artificial Neural Network (ANNs) models have found significant
applications towards bioinformatics data analysis. Various types of omics
datasets have been used for training ANNs and machine learning inference,
including for example protein or gene interaction networks, molecular
structures and cellular signalling pathways.  Unlike images, video and text
that are commonly used in the computer science and engineering fields and which
contain regular data structures, bioinformatics datasets present unique
challenges and require different algorithmic approaches.  The recent
development of the geometric deep learning approach within the machine learning
field, is promising towards accelerating scientific discovery also in
bioinformatics by enabling new approaches to understand complex datasets.  This
novel approach and its importance for bioinformatics machine learning is
demonstrated herein, through presentation of the undelying mathematical
foundations from group theory, graph data structures and linear algebra.
Furthermore, the structure and functions of ANNs algorithms that form the core
principles if artificial intelligence are explained, in relation to the
bioinformatics data domain.  Finally, we conclude the manuscript with a brief
discussion on the available coding frameworks, providing guidance for
researchers to transition from the mathematical principles to practical machine
learning software for bioinformatics applications.


== INTRODUCTION

Symmetry and invariance is a central concept in physical, mathematical and
biological systems, in addition to other areas of human endeavor such as social
networks. It has been established since the early 20th century that fundamental
laws of nature originate in symmetry, for example through the work of Noether
and Wigner cite:[noether1918invariante].In the last decade, technologies such
as genomic sequencing have enabled an exponential increase
cite:[katz2022sequence] of the data that describe the fundamental elements,
structure and function of biological systems. Other endeavors from fields as
diverse as physics, and the rise of social media platforms
cite:[clissa2022survey], have resulted in datasets of scale not previously
available to scientists. This data explosion, has also been fundamental for the
ever accelerating advancements in the field of machine learning, deep learning
and artificial intelligence, where we now  have algorithms that can be trained
to make discoveries from the data at a level that matches closely human
intuition.

As in any field (including bioinformatics) that develops rapidly whithin the
span of a few years,deep learning and artificial intelligence researchers have
developed hundreds of successful algorithms, however with a few unifying
principles . In a seminal `proto-book` by Bronstein et al.
cite:[bronstein2021geometric], a range of systematization principles for the
different artificial neural network architectures from the deep learning field
was presented, based on the concepts of symmetry that is formalized within the
mathematical field of group theory. The authors also introduced the concept of
Geometric Deep Learning, and demonstrated how the group theoretic principles of
iso- and auto-morphism, along with invariance and equivariance of functions,
can be used in composing and describing various deep learning algorithms. 

== THE STRUCTURE OF ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS

Before proceeding towards introducing these concepts further, we will describe
the structure and function of deep learning and Artificial Neural Networks
(ANNs) that are the foundation of artificial intelligence
cite:[li2019deep],through a simple mathematical description. Assume a dataset
consisting of _n_ pairs of stem:[(x_i,y_i)_n], with the stem:[x_i] being _n_
data points and stem:[y_i] their labels. Each stem:[x_i] data point can be for
example be a number, a vector (array of numbers), a matrix (grid of numbers)
storing the data for image pixels, or graph data structures composed by nodes
and edges representing drugs or other chemical molecules. The data and labels can
be of various formats, such as binary (two-option) as for example stem:[y_i=1]
"inhibits cancer growth", or stem:[y_i=0] "does not inhibit cancer". The labels
can also be continuous numbers such as for example stem:[y_i=0.3] meaning 30%
inhibition, while also each stem:[y_i] could be a composite label such as
stem:[y_i = [0,1,0]] representing respectively drug attributes such as 'no
inhibition', 'yes for toxicity', 'not metabolized', with 'not' for 0 and 'yes'
1. Similarly the data points can also be composite such as for example stem:[x_i = [50,100]]
representing two measuments for a single biological entity. 


The simplest structure of an artificial neural network as shown on *Fig.1* is
"fully connected", with each neuron _k_ in the network having a number of
incoming and outgoing connections corresponding to the number of neurons in
previous and next layer in the neural network. For example the first neuron
stem:[(k_0)^1] of Layer 1 on *Fig.1*, has stem:[n_in = 2] incoming and
stem:[n_out = 3] outgoing connections, corresponding respectively to first
"Input Layer" with two neurons for composite data of the form stem:[x_i =
[50,100]] as mentioned previously, and three required connections with the
neurons of the subsequent Layer 2.  Similar to a brain neural network, the
computational abstractions used in machine learning and artificial
intelligence, model neurons as signal aggregators and thresholding units.
Specifically, each artificial neuron performs a summation of incoming signals
from its connected neighbooring neurons in the network, and "fires" when the
aggregated signals reach a certain threshold. The signals can be either data
from the "Input Layer", or signals generated through computation in the
intermediate layers neurons as described next. The computation within each
neuron is represented with the summation stem:[sigma_(k)=sum_1^k w_(k)**x_(k) -
b] (*Fig.1*), where the stem:[w_(k)] represents the connection weight of
neighboring neuron _k_.  The next term stem:[x_(k)] multiplied by the weight
strength, is the incoming signal from the neigbooring neuron for which the
weighted connection is established. For example, the neuron stem:[k_0^1] of
Layer 1, will perform internaly the computation stem:[sigma_(k_0^1)=sum_1^2
(w_(k)**x_(k) + w_(k)**x_(k)) - b] As it will be mentioned in a subsquent
section, the weight value is the only variable element in the ANNs that is
adjusted by the algorithms in order to fit the data, similarly to the
biological brain where learning takes place by strengthening connections among
neurons. Unlike the biological brain, the ANNs used in practice for data
analysis have fixed connections between the neurons and structure of the neural
network. The last term _b_ in the summation, represents a threshold that needs
to be surpassed as stem:[sum_1^k w_(k)**x_(k) > b], in order for the neuron to
activate and "fire" a signal. 

Finally, the output value of the neuron, is determined by applying to the
summation value a thresholding or otherwise "logit" function
stem:[varphi(sigma_(k))] which is non-linear cite:[li2019deep].   This final
value is the output of the firing neuron, and  is transmitted to its connected
neurons in the next layer through the outgoing connections (or "axons" in the
biological analogy).  Multiple layers of such computational models of neurons
connected together in layers (*Fig.1*), along with multiple connections per
layer each having each own weight stem:[w_(k)], forms an Artificial Neural
Network (ANN).

[#img-fig1] .stem:[sigma_(123)=sum_1^3 w_1**x_1 + w_2**x_2 + w_3**x_3 - b]
image::graphviz.svg[Fig,659,361]

Mathematically, a trained ANN is a function stem:[f] that predicts the labels
stem:[y_(pred_i)] for the input data stem:[x_i], such as for example 'no
inhibition', 'yes for toxicity' etc. for data representing drug
molecules.  The function stem:[f] is non-linear and is estimated by fitting a
training dataset, which correlates labels stem:[y_i] to data points stem:[x_i].
Most important, stem:[f] is a composition of the stem:[varphi(sigma_(k))]
thresholding functions of the each neuron interconnected in the artificial
network graph cite:[li2019deep]. This is key aspect of the ANNs, since the
non-linearities present in each individual logit neuron function
stem:[sigma_(k)], are aggregated across the neural network and on the function
stem:[f], which enables fitting complex multi-dimensional input data with
non-linear distributions. This is the key fact that enables ANNs to achieve
higher clasification power compared to traditional statistical methods
cite:[tang2019recent]. With hundreds of papers and monographs that have been
written on the technical details of training ANNs,and since the focus of the
present review manuscript are networks designed with graph and other
non-euclidean datasets, we will next attempt to summarize the training in few
sentences and refer the reader to the citations for further details. 

In summary, the ANN algorithm is using the training data to identify a function
stem:[f] that predicts labels from the data such as stem:[y_(pred_i)=f(x_i)].
As mentioned previously, stem:[f] is a composition of the
stem:[varphi(sigma_((k_0)^1))] functions of the each neuron in the ANN, and as such
the training is the estimation of the weights stem:[w_k] of the neuron
connections, while minimizing the error stem:[E] based on the difference
between stem:[y_(pred_i)] and stem:[y_i].  This error quantifies the neural
network precision by comparing predicted stem:[y_(pred_i)] and actual labels
stem:[y_i]. The neuron connection weight stem:[w_k] estimation by the algorithm
takes place through fitting the network function stem:[f] on a large training
dataset of stem:[{x_i,y_i}_i^n], pairs of input data and labels, while it
evaluates the error stem:[E] using smaller testing and validation data subsets.
The training algorithm works by making an initial estimated guess for
initializing the weights, and then performing multiple cycles (called "epochs")
of fitting stem:[x_i] the training data to the network. At the end of each
cycle "backpropagation" is performed cite:[tang2019recent], which involves
gradient descent optimization, in order to fine tune the weights of the
individual neurons in stem:[sigma_(k)=sum_(k=1)^n w_(k)**x_(k) + b].  The
gradient descent (REF) searches the possible combinations of weight values, and
since it is a heuristic algorithm it minimizes but cannot reach zero error
stem:[E]. At the completion of multiple training cycles the training algorithm
identifies a set of weights which best fit the data model, and the ANN settles
on the optimal parameters that estimate the stem:[varphi(sigma_(k))] function
for each interconnected neuron.  Consequently, the overall stem:[f(x_i)] is
also estimated,since it is the composition of the individual
stem:[varphi(sigma_(k))] neuron functions.  Once the artificial neural network
training has ben completed by finding the most optimal set of weights, it is
now ready to be used for label prediction with new, unknown stem:[x_i] data.

== ARTIFICIAL INTELLIGENCE, GROUP THEORY, SYMMETRY AND INVARIANCE

We conclude, by briefly reviewing how the principles of group theory,
symmetry and invariance, are a foundational framework to understand the
function of machine learning algorithms, and examine the classifying power of
ANNs in relation to statistical variance and non-homogeneity in the data. In
summary, symmetry is the study of space and structure, with examples
referring to to geometric and algebraic constructs in mathematics, material
elements in physics and molecular biology structures. Invariance of an object
under transformation, is the property of changing the position of the object
in space, such as shifting a drug molecule or rotating a cancer histology
image, while leaving the properties of the object unchanged
cite:[bronstein2021geometric]. In these examples, the drug remains potent
following rotation of the molecule, and the tissue is still recognized as
cancerous based on the histology image. 

Following the terminology of Bronstein et al., we consider the input
stem:[x_i] from a data domain stem:[Omega], which has a specific structure
corresponding to the data type used for training the ANN. For example,
microscopy images are essentially 2-dimensional numerical grids (matrices) of
_n x n_ pixels, with each pixel having a value for light intensity.  In this
case the data domain is composed of integers (stem:[ZZ]) as grid stem:[Omega:
ZZ_n xx ZZ_n], which can have all possible combinations of pixel intensities.
Similarly, for color images the data domain is stem:[x_i:Omega to ZZ_n^3 xx
ZZ_n^3], with 3 integer grids each representing the green, blue and red
layers composing the color image. The ANN data fitting and label prediction
function stem:[y_(pred_i)=f(x_i)] is applied on a "signal" stem:["X"(Omega)]
from the domain, which is a subset of the domain stem:[Omega] with the
specific images used for training the neural network. 

Another important data structure for bioinformatics is a _graph_
latexmath:[$\gG = (\gV, \gE)$] that is composed of _nodes_ latexmath:[$\gV$]
representing biological entities, and _edges_  which are connections between
pairs of nodes (**Fig.2**).  In a specific instance of a graph, the present
edges are a subset of all possible edges between nodes.An example graph data
structure for a biological molecule such a protein or a drug, would represent
respectively the amino acids or atoms as node entities, and the chemical
bonds between each of these entities as edges. Noted that the nodes and their
edge connections are simply an abstraction of the real-world object, and the
edges can correspond to either the carbonyl-amino (C-N) peptide bonds between
amino acids, molecular interactions across the peptide chain leading to three
dimensional protein structure, or the bonds in the chemical structure of a
small molecule in the preceding examples. Next, attributes in the source data
such as for example polarity and amino acid weight , or drug binding
properties of the chemical molecule can be represented as node attributes
latexmath:[$s$]-dimensional , where _s_ are the number of attributes in the
data for each object represented as a node. Similarly the edges or entire
graphs, can have attributes, for data on the molecular interactions
represented by the edges or the whole molecular entity (ex.  protein or
drug).

From a mathematical and algorithmic perspective, images are a special case of
graphs where the nodes have a set of connection with edges in a structured
pattern that form of a grid. Under this perspective, the graph nodes are the
pixels of the images, and the edges the connections specifying the adjacency
of the pixels (**Fig.2**).  With this realization in place, we can now
examine the analytical and classification power of ANNs given variance in the
data, for both data types in cases where we have in the dataset shifted or
rotated input images or molecules represented as graphs.  We establish this
through the principles of group theory, symmetry and invariance. These are
the foundational mathematical and algorithmic principles that model the
performance and output of machine learning algorithms ANNs in relation to the
variability in the dataset. Consecutively, these principles can then be
extrapolated to for other types of data beyond graphs and images, 
for which ANNs are trained for prediction and classification.

A _symmetry group_ latexmath:[$\fG$] can be defined between the the input
dataset used for training the ANN, which is a subset "signal" stem:["X"(Omega)]
of all possible images and graphs from a data domain stem:[Omega] that can be
formed for example on a  grid (**Fig.2**). The members of the symmetry group
latexmath:[$\fg \in \fG$] are the associations of two or more points on the
grid latexmath:[$u,v\in \Omega$] between which an image or graph can be
rotated, shifted etc. without distortion of the data.  during .  which are of
the perform the group operations graph and image objects. Therefore, the key
aspect of the formal mathematical definition of the group, is that the data
attributes are preserved so that an image for example is not distorted when
moved on the plane, or similarly not changing the connnections between the
graph edges and nodes representing molecule elements, when the molecule is
rotated. something that is common in noisy, real-world data. .  Essentially,
the symmetry group ensures through the association of that the This is an
important aspect of modeling data classification and training of ANNs through
group theory and symmetry, so we can formalize machine learning perfomance.
Here we presented the group notion with a more data-centric definition which
nonetheless follows the mathematical formalism, where we do not specify what
the group operations but only how they can transform the input data. Therefore,
different types of data can have the same symmetry group, where transformations
of different types of data are performed by the same group operation. For
example, an image with a triangle (or a graph with three nodes), can have the
same rotational symmetry group as a graph of three nodes or a numerical
sequence of three elements.  While in this example we use a simple image or
graph, as 

NEED EDITING FROM HERE ON

latexmath:[$\fg, \fh \in \fG$] and latexmath:[$u
\in \Omega$]. We shall see numerous instances of group actions in the following
sections. For example, in the plane the _Euclidean group_
latexmath:[$\mathrm{E}(2)$] is the group of transformations of
latexmath:[$\R^2$] that preserves Euclidean distances, and consists of
translations, rotations, and reflections. this is SYMMETRY More precisely, if
we have a group latexmath:[$\fG$] acting on latexmath:[$\Omega$], we
automatically obtain an action of latexmath:[$\fG$] on the space
latexmath:[$\mathcal{X}(\Omega)$]:

latexmath:[$\mathbf{x}_u$] for all latexmath:[$u \in \gV$].  The key structural
property of graphs is that the nodes in latexmath:[$\gV$] are usually not
assumed to be provided in any particular order, and thus any operations
performed on graphs should not depend on the ordering of nodes. The desirable
property that functions acting on graphs should satisfy is thus _permutation
invariance_, and it implies that for any two _isomorphic_ graphs, the outcomes
of these functions are identical.  

Since both examples of the image and graphs are similarly grids on a two
dimemnsional plane, we could use a linear representation of group and represent
the group transformations in a matrix algebra format that is multiplied 


[latexmath] ++++ \[(\fg . x)(u) = x(\fg^{-1} u).  \label{eq:group_action}\]
++++ Due to the inverse on latexmath:[$\fg$], this is indeed a valid group
action, in that we have latexmath:[$(\fg . (\fh . x))(u) = ((\fg \fh) .
x)(u)$].

The most important kind of group actions, which we will encounter repeatedly
throughout this text, are _linear_ group actions, also known as _group
representations_. The action on signals in
equation (#eq:group_action[[eq:group_action]]) is indeed linear, in the sense
that

[latexmath] ++++ \[\fg . (\alpha x + \beta x') = \alpha (\fg . x) + \beta (\fg
. x')\] ++++ for any scalars latexmath:[$\alpha, \beta$] and signals
latexmath:[$x, x' \in \mathcal{X}(\Omega)$]. We can describe linear actions
either as maps latexmath:[$(\fg, x) \mapsto \fg.x$] that are linear in
latexmath:[$x$], or equivalently, by currying, as a map latexmath:[$\rho : \fG
\rightarrow \R^{n \times n}$] that assigns to each group element
latexmath:[$\fg$] an (invertible) matrix latexmath:[$\rho(\fg)$]. The dimension
latexmath:[$n$] of the matrix is in general arbitrary and not necessarily
related to the dimensionality of the group or the dimensionality of
latexmath:[$\Omega$], but in applications to deep learning latexmath:[$n$] will
usually be the dimensionality of the feature space on which the group acts. For
instance, we may have the group of 2D translations acting on a space of images
with latexmath:[$n$] pixels.

As with a general group action, the assignment of matrices to group elements
should be compatible with the group action. More specifically, the matrix
representing a composite group element latexmath:[$\fg \fh$] should equal the
matrix product of the representation of latexmath:[$\fg$] and
latexmath:[$\fh$]:

A latexmath:[$n$]-dimensional real _representation_ of a group
latexmath:[$\fG$] is a map latexmath:[$\rho : \fG \rightarrow \R^{n \times
n}$], assigning to each latexmath:[$\fg \in \fG$] an _invertible_ matrix
latexmath:[$\rho(\fg)$], and satisfying the condition latexmath:[$\rho(\fg \fh)
= \rho(\fg) \rho(\fh)$] for all latexmath:[$\fg, \fh \in \fG$]. A
representation is called _unitary_ or _orthogonal_ if the matrix
latexmath:[$\rho(\fg)$] is unitary or orthogonal for all latexmath:[$\fg \in
\fG$].

Written in the language of group representations, the action of
latexmath:[$\fG$] on signals latexmath:[$x \in \mathcal{X}(\Omega)$] is defined
as latexmath:[$\rho(\fg) x(u) = x(\fg^{-1} u)$]. We again verify that

[latexmath] ++++ \[(\rho(\fg) (\rho(\fh) x))(u) = (\rho(\fg \fh) x)(u).\] ++++

image:figures/geom_prior.png[ Three spaces of interest in Geometric Deep
Learning: the (physical) _domain_ latexmath:[$\Omega$], the space of
_signals_ latexmath:[$\mathcal{X}(\Omega)$], and the _hypothesis_ class
latexmath:[$\mathcal{F}(\mathcal{X}(\Omega))$]. Symmetries of the domain
latexmath:[$\Omega$] (captured by the group latexmath:[$\fG$]) act on
signals latexmath:[$x\in \mathcal{X}(\Omega)$] through group
representations latexmath:[$\rho(\fg)$], imposing structure on the
functions latexmath:[$f\in \mathcal{F}(\mathcal{X}(\Omega))$] acting on
such signals. ]

When artificial neural networks act as function estimators stem:[f("X"(Omega)
to "Y")] to predict output labels (i.e stem:[y_i] = potent drug / not potent, 

Such transformations may be either smooth, continuous, or discrete. Write here
about homeomorphisms, homorphisms etc.

bibliography::[]

