== Principles of Artificial Neural Networks and Machine Learning for Bioinformatics Applications

Konstantinos Krampis*^1^, Eric Ross^2^, Olorunseun O. Ogunwobi^1^, Grace Ma,^3^ Raja Mazumder,^4^ Claudia Wultsch^1^


:stem:

^1^Belfer Research Facility, Biological Sciences, Hunter College, City University of New York, NY, USA
^2^Fox Chase Cancer Center, Philadephia, PA, USA
^3^Center for Asian Health, Lewis Katz School of Medicine, Temple University, Philadelphia, PA, USA
^4^Biochemistry and Molecular Biology, George Washington University, Washington D.C., USA

^*^Corresponding Author, _kk104@hunter.cuny.edu_


=== ABSTRACT
With the exponential growth of machine learning and development of Artificial
Neural Network (ANNs) in recent years, there is great opportunity to leverage
this approach and accelarate biological discoveries through applications in the
analysis of high-throughput data. Various types of datasets, including protein
or gene interaction networks, molecular structures, and cellular signalling
pathways, have already been utilized for machine learning by training ANNs for
inference and pattern classification. However, unlike regular data structures
commonly used in the fields of computer science and engineering, bioinformatics
datasets present challenges that require unique algorithmic approaches. The
recent development of geometric and deep learning approaches within the machine
learning field holds great promise for accelerating the analysis of complex
bioinformatics datasets. Here, we demonstrate the principles of ANNs and their
significance for bioinformatics machine learning by presenting the underlying
mathematical and statistical foundations from group theory, symmetry, and
linear algebra. Furthermore, the structure and functions of ANN algorithms,
which constitute the core principles of artificial intelligence, are explained
in relation to the bioinformatics data domain. In summary, this manuscript
provides guidance for researchers to understand the principles necessary for
practicing machine learning and artificial intelligence, with special
considerations for bioinformatics applications.

*Keywords:*_machine learning, artificial intelligence, bioinformatics, cancer biology, neural networks, symmetry, group theory, algorithms_
biology, neural networks, symmetry, group theory, algorithms_


=== SIMPLE SUMMARY
Here, we provide an overview of the foundational formalisms of Artificial
Neural Networks (ANNs), which serve as the basis for Artificial Intelligence
within the broader field of of Machine Learning.  The review is from the
perspective of bioinformatics data, and multiple examples showcasing the
applications of these formalisms to experimental scenarios are presented
herein. The mathematical formalisms are explained in detail, offering
biologists who are not Machine Learning experts the opportunity to understand
the algorithmic basis of Artificial Intelligence as it relates to
bioinformatics applications.

=== INTRODUCTION
In summary, Artificial Intelligence (AI), Machine Learning (ML), and Deep
Learning (DL) are interconnected concepts with distinct differences: AI is
centered around developing machines capable of performing tasks that require
human intelligence, ML empowers computers to learn from data and make
predictions without explicit programming, and DL employs deep neural networks
to discern patterns from complex datasets. AI encompasses both ML and DL, which
function as subsets of AI. ML algorithms learn patterns from data to facilitate
accurate predictions or decisions and can be categorized into supervised,
unsupervised, and reinforcement learning. DL algorithms, drawing inspiration
from the human brain, utilize deep neural networks to learn and extract
patterns from large-scale datasets. DL has shown success in domains such as
image and speech recognition, natural language processing (NLP), and autonomous
driving.

In the last decade, technologies such as genomic sequencing have led to an
exponential increase cite:[katz2022sequence] in the data describing the
molecular elements, structure, and function of biological systems.
Additionally, data digitization and generation across varied fields such as
physics, software development, and social media cite:[clissa2022survey],
has yielded complex datasets of scales previously unavailable to scientists. AI
also provides many opportunities for healthcare, ranging from clinical
decision-support systems to deep-learning based health information management
systems. This abundance of data has played a pivotal role in the rapid progress of
machine learning, deep learning, and artificial intelligence. As a result, we
now have algorithms that can be trained to extract insights from data with a
level of sophistication that closely resembles human intuition.

While researchers have developed hundreds of successful algorithms, there are
currently a few overarching principles to systematically organize machine
learning algorithms. In a seminal `proto-book` by Bronstein et al.
cite:[bronstein2021geometric], various systematization principles for different
Artificial Neural Network (ANN) architectures and deep learning algorithms were
presented. These principles are founded on the concepts of symmetry and
mathematical group theory. Symmetry and invariance are central concepts in
physics, mathematics, and biological systems. Since the early 20^th^ century,
it has been established that fundamental principles of nature are rooted in
symmetry cite:[noether1918invariante]. The authors also introduced the concept
of geometric deep learning and demonstrated how group theory, along with
function invariance and equivariance principles, can serve as foundation for
composing and describing different deep learning algorithms. Along these lines,
the present manuscript explains the structure of ANNs and the core principles
of machine learning algorithms. Additionally, it offers a review of the
mathematical and statistical foundations pertinent to the development of
artificial intelligence applications using bioinformatics data.

=== THE STRUCTURE OF ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS
We will begin by describing the structures and functions of deep learning and
Artificial Neural Networks (ANNs), which form the foundation of artificial
intelligence cite:[li2019deep]. We use a dataset consisting of _n_ pairs of
latexmath:[$\left( x_{i},y_{i} \right)_{n}$], where latexmath:[$x_{i}$]
represents _n_ data points and latexmath:[$y_{i}$] their corresponding labels.
Each latexmath:[$x_{i}$] data point can take the form of a number, a vector (an
array of numbers), or a matrix (a grid of numbers), storing various types of
bioinformatics data. The labels can assume different formats, such as binary
(two-options), like latexmath:[$y_{i} = 1$] "inhibits cancer growth", or
latexmath:[$y_{i} = 0$] "does not inhibit cancer". The labels can also be
continuous numbers, for instance, latexmath:[$y_{i} = 0.3$] indicating 30%
inhibition, or a composite label such as latexmath:[$y_{i} = \left( 0,1,0
\right),$] which signifies drug attributes like '0 - no inhibition', '1 - yes
for toxicity', '0 - not metabolized', respectively. Similarly, the input data
points can also be composite, for example, latexmath:[$x_{i} = \left( 50,100
\right)$] representing two measuments for a single biological entity.
Regardless of the label structure, the primary objective of deep learning
algorithms and the overarching goal of artificial intelligence applications in
bioinformatics is to first train the ANN using data with known labels.
Subsequently, the ANN is utilized to classify newly generated data by
predicting their labels cite:[Nair2021].

The simplest structure of an artificial neural network, as depicted in *Fig.1,*
is "fully connected". In this structure, each neuron _k_ within the ANN
possesses a specific number of incoming and outgoing connections. These
connections correspond to the quantity of neurons present in the previous and
next layers within the neural network cite:[Nair2021]. For example, the neuron
latexmath:[$k_{1}^{(1)}$] of the _First Layer (1)_ on *Fig.1*, which has
latexmath:[$n = 2$] incoming and latexmath:[$n = 3$] outgoing connections.
These connections align with the "input layer", which comprises two neurons,
and the three connections extend to the neurons of the internal ("hidden
layer") denoted as _Second Layer (2)_ in the figure. The designation “hidden”
is attributed to the internal layers because they do not directly receive input
data. 

This concept parallels the behavior of neurons engaged in cognition within
animal brains, in contrast to sensory neurons. While the number of neurons in
the hidden layers can vary based on the complexity of the label classification
problem that the ANN is intended to address cite:[uzair2020effects], the input
layer must have a precise number of neurons that align with the structure of
the input data. In *Fig. 1,* for instance, there are two input neurons, and the
data can take the form latexmath:[$x_{i} = \left( 50,100 \right)$]. Lastly, the
output layer consists of a number of neurons corresponding to the count of
labels latexmath:[$y_{i}$] associated with each input data point in the
dataset. In *Fig. 1,* a single label is presented.


[.middle]
[graphviz, target=Fig1, format=svg]
....
digraph G {

    // ========================
    // rankdir: directed graph drawn from left to right 
    // details: http://www.graphviz.org/doc/info/attrs.html
    // ========================
    rankdir=LR;  
    edge[style=solid, tailport=e];
    nodesep=0.4;
    
    // ========================
    // splines=line:  draw straight lines to connect nodes
    // ========================
    splines=line;
    node [label=""];
    subgraph cluster_0 {
        label="Input layer";
        node [color=chartreuse, 
              style=filled, 
              shape=circle];
        x0 [fillcolor=chartreuse, 
            label=<X<sub>1</sub>>];
        x1 [fillcolor=chartreuse, 
            label=<X<sub>2</sub>>];
 
    }

    subgraph cluster_1 {
        color=white;
        label="First Layer (1)";
        node [color=dodgerblue, 
              style=filled, 
              shape=circle];
        a02 [fillcolor=dodgerblue, 
             label=<k<sub>1</sub><sup>(1)</sup>>];
        a12 [fillcolor=dodgerblue, 
             label=<k<sub>2</sub><sup>(1)</sup>>];
        a22 [fillcolor=dodgerblue, 
             label=<k<sub>3</sub><sup>(1)</sup>>];
    }

    subgraph cluster_2 {
        color=white;
        label="Second Layer (2)";
        node [color=dodgerblue, 
              style=filled, 
              shape=circle];
        a03 [fillcolor=dodgerblue, 
             label=<k<sub>3</sub><sup>(2)</sup>>];
        a13 [fillcolor=dodgerblue, 
             label=<k<sub>2</sub><sup>(2)</sup>>];
        a23 [fillcolor=dodgerblue, 
             label=<k<sub>1</sub><sup>(2)</sup>>];

    }

    subgraph cluster_3 {
 
        label="Output Layer";
        node [color=coral1, 
              style=filled, 
              shape=circle];
        O1 [fillcolor=coral1, 
            label=<Y<sub> </sub>>];
      

    }

   // ========================
   // This is the trick to enforce the bias node stays at the top of 
   // vertical array of nodes in each layer
   // style=invisible: makes the edge connection invisible
   // dir=none: hide the arrow 
   // ========================
    x0 -> a02 [penwidth=0.5];
    x0 -> a12 [penwidth=0.5];
    x0 -> a22 [penwidth=0.5];
    
    x1 -> a02 [penwidth=0.5];
    x1 -> a12 [penwidth=0.5];
    x1 -> a22 [penwidth=0.5];

    a02 -> a03 [penwidth=0.5];
    a02 -> a13 [label=<W<SUB>k1</SUB> * X<SUB>k1</SUB>>, fontcolor=blue, color=red, fontsize=10, penwidth=2.5];
    a02 -> a23 [penwidth=0.5];
 
    a12 -> a03 [penwidth=0.5];
    a12 -> a13 [label=<W<SUB>k2</SUB> * X<SUB>k2</SUB>>,fontcolor=blue, color=red, fontsize=10, penwidth=2.5 ];
    a12 -> a23 [penwidth=0.5];

    a22 -> a03 [penwidth=0.5];
    a22 -> a13 [label=<W<SUB>k3</SUB> * X<SUB>k3</SUB>>,fontcolor=blue, color=red, fontsize=10, penwidth=2.5];
    a22 -> a23 [penwidth=0.5];
 
    a03 -> O1 [penwidth=0.5];
    a13 -> O1 [penwidth=0.5];
    a23 -> O1 [penwidth=0.5];
}
....


''' 
*Figure 1.* An example *Artificial Neural Network (ANN)*. The signal
aggregation taking place on the second neuron
latexmath:[$\sigma_{k_{2}^{(2)}}$] of the second hidden layer, can be expressed
with the formula latexmath:[$\sigma_{k_{2}^{(2)}} =
\sum_{k_{1,2,3}}^{(\begin{matrix} 1 \\ \end{matrix})}w_{k1}*x_{k1} +
w_{k2}*x_{k2} + w_{k3}*x_{k3} - b$], which is the aggregation of neuron signals
from the first layer, shown as red arrows in the figure. _b_ represents the
threshold that needs to be overcome by the aggregation sum in order for the
neuron to fire, and then the neuron will transmit a signal along the line shown
towards the output on the final layer of the figure. The reader should refer to
the text for more details.  
'''

Similar to neural networks in animal brains, the computational abstractions
used in machine learning and artificial intelligence model neurons as
computational units that execute signal summation and threshold activation
cite:[Renganathan2019]. Specifically, each artificial neuron performs a
summation of incoming signals from its connected neighbooring neurons in the
preceding layer on the network, shown for example as red arrows on *Fig.1* for
latexmath:[$\sigma_{k_{2}^{(2)}}$] . The signal processing throughout the ANN
transitions from the input data latexmath:[$x_{i}$] on the leftmost layer
(*Fig.1*) to the output of data labels latexmath:[$y_{i}$] on the rightmost
end.  Within each neuron, when the aggregated input reaches a certain
threshold, the neuron "fires" and transmits a signal to the subsequent layer.

The signals entering the neuron can either be the data directly from the input
layer or signals generated by the activation of neurons in the intermediate
"hidden" layers. The summation and thresholding computation within each neuron
is represented with the function latexmath:[$\sigma_{k} =
\sum_{1}^{k}w_{k}*x_{k} - b$], where latexmath:[$w_{k}$] represents the
connection weights of the preceding neurons.  Each connection arrow in *Fig.1*
has a distinct weight, such as, for example, latexmath:[$x_{k1}$] which is the
incoming signal from the neuron latexmath:[$\sigma_{k_{1}^{(1)}}$]  to neuron
latexmath:[$\sigma_{k_{2}^{(2)}}$] , multiplied by the weight
latexmath:[$w_{k1}$], which symbolizes the strength of the connection between
these two artificial neurons.

The weights in artificial neural networks embody the strength of connections
between neurons. They determine the impact of input signals on the final output
of the network. Throughout the training process, these weights are adjusted to
minimize the difference between the network’s predicted and intended output.
The weights govern the information flow within the network, enabling it to
learn and generate precise predictions. Accurately calibrated weights are
crucial for the network to effectively learn patterns and extrapolate its
knowledge to novel input data cite:[Renganathan2019].

For the majority of applications, the weight values latexmath:[$w_{k}$]
constitute the only elements in the ANN structure that are variable and
adjusted by the algorithms during training using the input data. This process
is similar to the biological brain, where learning takes place by strengthening
connections among neurons cite:[wainberg2018deep].  However, unlike the
biological brain, the ANNs used for practical data analysis have fixed
connections between neurons and the structure of the neural network remains
unaltered during the process of training and learning to recognize and classify
new data. The last term _b_ in the summation signifies a threshold that must be
surpassed, as in latexmath:[$\sum_{1}^{k}w_{k}*x_{k} > b$], to trigger the
activation of a neuron. 

A final step prior to transmitting the neuron’s output value involves the
application of a "logit" function to the summation value that is represented as
latexmath:[$\varphi\left( \sigma_{k} \right)$].  latexmath:[$\varphi$] can be
selected from a range of non-linear functions contingent on the type of input
data and the specific analysis and data classification domain for which the ANN
will be used cite:[li2019deep]. The value of the logit function is the output
of the neuron, which is transmitted to its interconnected neurons in the
subsequent layer through outgoing connections, illustrated as an arrow in
*Fig.1* and corresponding to the brain cell axons in the biological analogy.
Multiple layers of interconnected neurons (*Fig.1*), along with multiple
connections per layer, each having its own weight latexmath:[$w_{k}$], together
form the framework of the Artificial Neural Network (ANN).

From a mathematical formalism perspective, a trained ANN is a function
latexmath:[$f$] that predicts labels latexmath:[$y_{\text{pre}d_{i}}$], which
can include categories such as 'no inhibition', 'yes for toxicity' etc., for
different types of input data latexmath:[$x_{i}$], ranging from histology
images to drug molecules represented as graph data structures. Therefore, the
ANN undertakes data classification by operating as a mapping function
latexmath:[$f\left( x_{i} \right) = y_{\text{pre}d_{i}}$], that connects the
input data to the respective labels. Furthermore, the latexmath:[$f\left( x_{i}
\right)$] is a non-linear function, since it is an aggregate composition of the
non-linear functions latexmath:[$\varphi\left( \sigma_{k} \right)$] of the
individual interconnected neurons within the network cite:[li2019deep]. As
a result, the latexmath:[$f\left( x_{i} \right)$] can successfully classify
labels for data inputs originating from complex data distributions. This fact
enables ANNs to attain heightened analytical capability compared to
conventional statistical learning algorithms cite:[tang2019recent]. The
latexmath:[$f\left( x_{i} \right)$] estimation is carried out by fitting a training
dataset, which establishes correlations between labels latexmath:[$y_{i}$] and
data points latexmath:[$x_{i}$]. With hundreds of papers and monographs that
were written on the technical details of training ANNs, we will next attempt to
briefly summarize the process and direct the reader to provided citations for
further details cite:[Zou2008a].

As mentioned earlier, the only variable elements in the ANN structure are the
weights latexmath:[$w_{k}$] of neuron connections. Therefore, training an ANN
to classify data involves the estimation of these weights. Furthermore, the
training process entails minimizing the error latexmath:[$E$], which is the
difference between the labels latexmath:[$y_{\text{pre}d_{i}}$] predicted by
the function latexmath:[$f$] and the true labels latexmath:[$y_{i}$]. This
error metric is akin to true/false positive and negatives (precision and
recall) used in statistics, however, different formulas are used for its
estimation when dealing with multi-label or complex input data for the ANN (for
further details, refer to cite:[kriegeskorte2019neural]).  The estimation of
neuron connection weights latexmath:[$w_{k}$] is executed by the algorithm
through fitting the network function latexmath:[$f$] to a large training
dataset of latexmath:[$\left\{ x_{i},y_{i} \right\}_{i}^{n}$] pairs of input
data and labels, while the error latexmath:[$E$] is calculated by using a
subset of the data for testing and validation purposes. The training algorithm
starts with an initial value of the weights, and then performs multiple cycles,
referred to as "epochs", to estimate the function latexmath:[$f.$] This is
achieved by fitting the data latexmath:[$x_{i}$] to the network and calculating
the error latexmath:[$E$] by comparison between the predicted
latexmath:[$y_{\text{pre}d_{i}}$] and the true labels latexmath:[$y_{i}$]. At
the end of each cycle, a process called "backpropagation" is performed
cite:[tang2019recent], which involves a gradient descent optimization
algorithm, which fine-tunes the weights of individual neurons to minimize
latexmath:[$E$]. 

The gradient descent cite:[ruder2016overview] optimization examines a large
subset of all possible combinations of weight values, yet as a heuristic
algorithm, it minimizes latexmath:[$E$], but cannot reach zero error. Upon the
completion of multiple training cycles, the training algorithm identifies a set
of weights that best fit the data with minimal error. The ANN settles on the optimal values that
estimate each latexmath:[$\varphi\left( \sigma_{k} \right)$] function for
latexmath:[$\sigma_{k} = \sum_{1}^{k}w_{k}*x_{k} - b$], where
latexmath:[$w_{k}$] is the weight in each interconnected neuron.  Consequently,
the overall function latexmath:[$f$] represented by the network is also
estimated, as it comprises the composition of the individual
latexmath:[$\varphi\left( \sigma_{k} \right)$] neuron functions, as mentioned
earlier. Following the completion of the artificial neural network training,
where the most optimal set of weights is determined, the network is ready to be
used for label prediction with new, unknown latexmath:[$x_{i}$] data.

=== ARTIFICIAL INTELLIGENCE, GROUP THEORY, SYMMETRY AND INVARIANCE

==== Data domains in relation to group theory and symmetry

In the remaining sections, we will examine how the principles of group theory, symmetry,
and invariance provide a foundational framework for comprehending the function
of machine learning algorwthms. Furthermore, the classifying power of ANNs, particularly
in relation to statistical variance, transformations, and non-homogeneity in
the input data. In broad terms, symmetry entails the analysis of geometric and
algebraic mathematical structures and finds applications across different
research fields, including physics, molecular biology, and machine learning. A
core concept in symmetry is invariance, which, in our context, is changing data
coordinates, such as relocating a drug molecule in space or shifting the
position of a cancer histology tissue sample, while maintaining the shape of
the object unchanged cite:[bronstein2021geometric]. Following such an
alteration, which will be formally defined later in this text as an _invariant
transformation_, it becomes imperative for the machine learning algorithms and
ANNs to be capable of identifying a drug molecule even after rotation or
recognizing cancerous tissue from a shifted histology image.

In order to link the abstract symmetry concepts with data classification in
machine learning, as per the terminology of Bronstein et al., we consider the
input data latexmath:[$x_{i}$] to originate from a symmetry domain denoted as
latexmath:[$\Omega$]. This latexmath:[$\Omega$] serves as the foundational
structure upon which the data are based, and it is upon this domain structure
that we train artificial neural networks to undertake classification, employing
the label prediction function latexmath:[$f$] as mentioned in the earlier
section. For example, microscopy images are essentially 2-dimensional numerical
grids of _n x n_ pixels (*Fig.2a*), with each pixel having an assigned value
corresponding to the light intensity captured when the image was taken.  

In this scenario, the data domain is a grid of integers
(latexmath:[$\mathbb{Z}$]), represented as latexmath:[$\Omega:\mathbb{Z}_{n}
\times \mathbb{Z}_{n}$]. Similarly, for color images, the data domain is
latexmath:[$\left. \ x_{i}:\Omega \rightarrow \mathbb{Z}_{n}^{3} \times
\mathbb{Z}_{n}^{3} \right.\ $], encompassing three overlaid integer grids that
individually represent the green, bluem and red layers composing the color
image cite:[Chartrand2017]. In either case, the latexmath:[$\Omega$] contains
all possible combinations of pixel intensities, while the specific pixel value
combinations of the images in the input data latexmath:[$x_{i}$] are a "signal"
latexmath:[$\text{X}\left( \Omega \right)$] from the domain. The ANN’s data
classification and label prediction function latexmath:[$y_{\text{pre}d_{i}} =
f\left( x_{i} \right)$] is applied upon the signal latexmath:[$\text{X}\left(
\Omega \right),$] which fundamentally constitutes a subset of the domain
latexmath:[$\Omega$].

A _symmetry group_ latexmath:[$G$] contains all possible transformations of the
input signal latexmath:[$\text{X}\left( \Omega \right),$] referred to as
symmetries latexmath:[$g$] or _group actions_. A symmetry transformation
latexmath:[$g$] preserves the properties of the data; for instance, it ensures
that objects within an image remain undistorted during rotation. The
constituents of the symmetry group, denoted as latexmath:[$g \in G,$] are the
associations of two or more coordinate points latexmath:[$u,v \in \Omega$] on
the data domain (grid in our image example). Between these coordinates, the
image can undergo rotation, shifting or other transformations without any
distortion.  

Consequently, the key aspect of the formal mathematical definition
of the group lies in its capacity to safeguard data attributes during object
distortions that frequently occur during the experimental acquisition of
bioinformatics data. The concept of symmetry groups is important for modeling
the performance of machine learning algorithms, particularly for classifying
the data patterns despite the variability inherently present within the input
data.

[.left]
[graphviz, target=Fig2a, format=svg]
....
digraph grid_layout {

  label="a. grid data for image pixels"
  node [shape=circle, style=filled, color=lightblue, fontname=Arial, fontsize=11];
  edge [color=gray, penwidth=1.5];

  A [label="Node A", color=green];
  B [label="Node B", color=blue];
  C [label="Node C", color=red];
  D [label="Node D", color=yellow];
  E [label="Node E", color=orange];
  F [label="Node F", color=purple];

  {rank=same; A; B; C;}
  {rank=same; D; E; F;}

  A -> B -> C;
  D -> E -> F;
  A -> D;
  B -> E;
  C -> F;
}
....


[.right]
[graphviz, target=Fig2b, format=svg]
....
digraph directedgraph {

  label="b. graph data structure for a protein or other molecule"
  rankdir=LR;  
  node [shape=circle, style=filled, color=lightblue, fontname=Arial, fontsize=11];
  edge [color=gray, penwidth=1.5];

  A [label="Node A", color=green];
  B [label="Node B", color=blue];
  C [label="Node C", color=red];
  D [label="Node D", color=yellow];
  E [label="Node E", color=orange];
  F [label="Node F", color=purple];

  A -> B;
  A -> C;
  B -> C;
  B -> D;
  C -> D;
  C -> E;
  D -> E;
  D -> F;
}
....


'''
*Figure 2. (a).* A _grid_ data structure representing image pixels, is 
formally a _graph_ *(b).* A _graph_ latexmath:[$G = (V, E)$], is composed of
_nodes_ latexmath:[$V$] shown as circles, and _edges_  connecting the nodes and
shown as arrows. It can represent a protein, where the amino acids are the
nodes and the peptide bonds between amino acids are the edges.

'''



Another important data structure within bioinformatics is a _graph_ denoted as
latexmath:[$G = (V,E)$], composed of _nodes_ latexmath:[$V$] that signify
biological entities, and _edges_ representing connections between pairs of
nodes (*Fig.* *2b*). In a specific instance of a graph corresponding to a
real-world object, the edges are a subset of all possible links between nodes.
An example graph data structure for a biological molecule such a protein or a
drug would portray the amino acids or atoms as node entities, while the
chemical bonds between each of these entities are captured as edges. These
edges could signify the carbonyl-amino (C-N) peptide bonds between amino acids
and molecular interactions across the peptide chain on the protein structure,
or the chemical bonds between atoms in a drug molecule
cite:[Kriegeskorte2019]. 

Furthermore, attributes in the molecular data such as, for example, polarity,
amino acid weight, or drug binding properties can be depicted as
latexmath:[$s$] - dimensional node attributes, where _s_ represents the
attributes assigned to each node.  Similarly, edges or even entire graphs can
have attributes, for experimental data measured on the molecular interactions
represented by the edges, and measurements of the properties of the complete
protein or drug. Finally, from an algorithmic perspective, images can be viewed
as a special case of graphs in which the pixels serve as nodes, interconnected
by edges following a structured pattern that generates a grid formation
(*Fig.2a*) representing the adjacent positions of the pixels.

==== Group theory and symmetry principles applied to machine learning

Having established the mathematical and algorithmic parallels between graphs
and images, we will now utilize the principles of the _symmetry group_
latexmath:[$G$] to examine the analytical and classification power of machine
learning ANNs, with respect to data variability and transformations. Whether it
involves data types like input images or molecules represented as graphs, which
may undergo shifts or rotations, we introduce the concept of invariance guided
by the principles of group theory and symmetry. These foundational mathematical
and algorithmic formalisms serve as the basis for modeling the performance and
output of machine learning algorithms, specifically ANNs, with regard to the
diversity present in the dataset. 

Consecutively, these principles can be extrapolated and generalized to
encompass other types of data beyond graphs and images, for which ANNs are
trained to predict and categorize.  While we present the group and symmetry
definitions following a data-centric approach, we will remain consistent with
the mathematical framework, while describing how the group operations can
effect transformations on the input data. Furthermore, different types of data
may have the same symmetry group, and different transformations could be
performed through identical group operations. For example, an image featuring a
triangle, which essentially is a graph with three nodes, might possess the same
rotational symmetry group as a graph with three nodes or a numerical sequence
of three elements.

When chemical and biological molecules are represented as graphs as described
earlier, the nodes latexmath:[$V$] can be in any order depending on how the
data were measured during the experiment. However, this variation does not
change the underlying information contained in the data. As long as the edges
*E,* which represent the connections between molecules, remain unchanged, we
maintain an accurate representation of the molecular entity, irrespective of
the sequence of nodes in *V*. In cases where two graphs portraying the same
molecule have identical edges but differ in node arrangement, they are called
_isomorphic_. It is crucial that any machine learning algorithm designed for
pattern recognition on graphs, should not depend on the ordering of nodes. This
ensures that classification using ANNs and artificial intelligence remain
robust against variations in experiment measurement encountered in real-world
data cite:[AgatonovicKustrin2000]. This is something that is taken for
granted with human intelligence, where, for example, we can recognize an object
even when a photograph is rotated at an angle. 

==== Invariance and the classification power of artificial neural networks

Returning to our earlier formal definitions of ANNs as function estimators
fitted to the data, in order for ANNs algorithms to equivalently recognize
_isomorphic_ graphs, the functions latexmath:[$\varphi\left( \sigma_{k}
\right)$] and overall latexmath:[$f\left( x_{i} \right)$] of the ANN acting on
graph data should be _permutation invariant_. This implies that for any
permutation of the input dataset, the output values of these functions remain
unchanged, regardless of the ordering of the nodes *V*. This concept can be
similarly applied to images, which, as previously mentioned, are specialized
instances of fully connected graphs.  Furthermore, these principles can also be
generalized for other data types beyond images or graphs.

To further formalize the concept of invariance, and considering that both image
and graph examples are essentially points on a grids on a two-dimemensional
plane, we can use linear algebra. Specifically, by using a matrix we can
represent the data transformations as group actions, denoted by
latexmath:[$g$], within the symmetry group latexmath:[$G$]. The use of matrices
enables us to connect the group symmetries with the actual data by performing
matrix multiplications that modify the coordinates of the object and
consecutively represent the data transformations through the multiplication.
The dimensions of the matrix, latexmath:[$n \times n,$] typically are similar
to these of the signal space latexmath:[$\text{X}\left( \Omega \right)$] for
the data (e.g., latexmath:[$\mathbb{Z}_{n} \times \mathbb{Z}_{n}$] images).
The matrix dimensions not depend on the size of the group (i.e. the number of
possible symmetries) or the dimensionality of the underlying data domain
latexmath:[$\Omega$]. With this definition in place, we can formalize
symmetries and group actions for modifying data objects, employing matrix and
linear transformations as the foundation for connecting invariance in relation
to variability in the data.

We will now conclude by establishing the mathematical and linear algebra
formalisms that underlie the resilience of ANNs and machine learning algorithms
in pattern recognition, considering transformations in the data. While our
framework is based on a two-dimensional grid data domain latexmath:[$\Omega$],
the formalisms developed here can also be extrapolated to any number of
dimensions or data formats without loss of generality. First, we will connect
matrices to group actions latexmath:[$g$] (such as rotations, shifts) within
the symmetry group latexmath:[$g \in G$] by defining a function
latexmath:[$\theta$] that maps the group to a matrix as latexmath:[$\theta:G
\rightarrow \mathbf{M}$]. As mentioned earlier, a matrix latexmath:[$\mathbf{M}
\in R^{n \times n}$] consisting of numerical values (integers, fractions,
positive and negative), when multiplied by the coordinate values of an object
on the plane latexmath:[$\Omega$], results in rotation or shifts of the
object’s coordinates for the exact amount corresponding to the group action
within the symmetry group.

With these definitions in place, we will now connect the matrix formalisms with
the neural network estimator function latexmath:[$y_{\text{pre}d_{i}} = f\left(
x_{i} \right)$], which is identified by adjusting neuron connection weights
during multiple training cycles with the input data. Our goal is to leverage
the mathematical formalisms of group symmetry and invariance to establish the
resilience of ANNs in classifying and assigning labels to new data points
cite:[Eetemadi2019]. These data points originate from real-world data that
might contain tranformations and distortions.  First, we define the estimator
function of the ANN to be _invariant_ if the condition for the input data
holds, i.e.  latexmath:[$f(\mathbf{M} \times x_{i}) = f(x_{i})$] for all
matrices latexmath:[$\mathbf{M}$] representing the actions latexmath:[$g \in
G$] within the symmetry group. 

This formula encapsulates the requirement for the neural network function to be
invariant: its output value remains the same whether the input data
latexmath:[$x_{i}$] are transformed or not (e.g., an image or graph is not
rotated on the plane), as represented by the matrix multiplication
latexmath:[$\mathbf{M} \times x_{i}$]. Therefore, the output values
latexmath:[$y_{\text{pre}d_{i}} = f\left( x_{i} \right)$] produced by the ANN,
which essentially represent predicted output labels (e.g.,
latexmath:[$y_{\text{pre}d_{i}}$] = potent drug / not potent), based on the
input data, exhibit resilience to noisy and deformed real-world data when the
network estimator function is invariant. In a different case, the estimator
function approximated by the ANN can be _equivariant_ and defined as
latexmath:[$f(\mathbf{M} \times x_{i}) = \mathbf{M} \times f(x_{i})$].  This
signifies that the output of the ANN will be modified, but the label prediction
result will shift equally alongside the shift in the input data.

==== Neural networks and group theory in relation to continuous data transformations 

Up to this point, we have exclusively discussed discrete tranformations in
linear algebra terms, utilizing matrix multiplications that lead to coordinate
shifts and rigid transformations of the data, like rotating an image or graph
by a specific angle on the grid latexmath:[$\Omega$].  However, in real-world
data scenarios, we often also encounter continuous, more fine-grained shifts.
In such cases, ANNs algorithms should be able to recognize patterns, classify,
and label the data without any loss of performance cite:[Wright2022].
Mathematically, the continuous transformations follow equally with the
invariant and equivariant functions described earlier. For instance, if the
domain latexmath:[$\Omega$] contains data with smooth transformations and
shifts, such as moving images (videos) or shifts of molecules and graphs that
maintain _continuity_ in a topological definition
cite:[sutherland2009introduction], in this case we deal with a concept
known as _homeomorphism_ instead of _invariance_.

Finally, if the rate of continuous transformation of the data is quantifiable,
meaning that the function latexmath:[$\theta,$] which maps the group to a
matrix, is _differentiable_, then the members of the symmetry groups will be
part of a _diffeomorphism_. As it follows from the principles of calculus, in
this case, infinitely multiple matrices latexmath:[$f(\mathbf{(}M)$] will be
needed to be produced by latexmath:[$\theta$] for the continuous change of the
data coordinates at every point. These differentiable data structures are
common with manifolds, which, for example, could be used to represent proteins
in fine detail. In this case, the molecule would be represented as a cloud with
all atomic forces surrounding the structure, as opposed to the discrete data
structure of nodes and edges in a graph. Finally, if the manifold structure
also includes a metric of _distance_ between its points to further quantify the
data transformations, in this case, we will have an _isometry_ during the
transformation due to a group action from the symmetry group.

=== APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS IN BIOINFORMATICS

Artificial Intelligence (AI) and Deep Learning have emerged as powerful tools
with diverse applications in the field of bioinformatics, and multiple research
studies have been reported in the literature cite:[pmid37446831],
cite:[pmid37189058], cite:[pmid37043378], highlighting the potential of
the technology to revolutionize healthcare and life sciences. One of the
significant applications is drug discovery, as AI algorithms facilitate the
analysis of large datasets of chemical compounds, predicting their
effectiveness and safety cite:[pmid37479540], cite:[pmid37458097],
cite:[pmid37454742]. These studies have demonstrated that AI can accelerate
the drug discovery process by screening potential candidates and optimizing
their properties, resulting in substantial cost and time savings.

In the field of genomics, AI algorithms have been applied to the analysis of
DNA sequencing and gene expression data, facilitating, for example, the
identification of disease-causing mutations and enhancing our understanding of
genetic variations cite:[pmid37453366], cite:[pmid37446311],
cite:[pmid37386009], cite:[pmid37370847].  Moreover, in these studies, genomic
data analysis with AI algorithms has provided critical insights, which can
assist in the development of personalized medicine approaches and as result
tailor treatments to individual patients. Consecutively, the use of AI
algorithms in bioinformatics can contribute to the advancement of precision
medicine.  By integratively analyzing also other omics data (e.g.,
transcriptomics, proteomics, metabolomics), patient data, encompassing genetic
information, medical history, and lifestyle factors, AI-driven insights can
lead to improved predictions of drug responses, identification of potential
side effects, and the recommendation of optimal treatment options for
individual patients.

This personalized medicine approach can also involve enhancing patient care and
treatment outcomes, through disease diagnosis improved by machine learning
analysis of medical images, including computed tomography (CT) and magnetic
resonance imaging (MRI) scans, X-rays, and histopathology images, of diseases
like cancer cite:[pmid37488621], cite:[pmid37478073], cite:[pmid37474003],
cite:[pmid37449611].  The AI algorithms can assist pathologists and
radiologists in rendering precise diagnoses, enabling early detection and
diagnosis, and ultimately contributing to overall improvements in patient
outcomes.

AI can also play a significant role in assisting the development of
bioinformatics tools and software accelerating the process of code development
for the analysis and interpretation of biological data, such as sequence
alignment, protein structure prediction, and functional annotation
cite:[pmid37329982], cite:[pmid37463768], cite:[pmid37460991].  Furthermore,
AI-powered natural language processing techniques have been
employed to analyze scientific literature, patents, and clinical trial reports.
This capability enables researchers to stay updated about the latest
discoveries and facilitates knowledge discovery in the field.

Finally, in the area of clinical trials, machine learning algorithms have been
appplied to mine vast amounts of data from clinical trials. As a result, the
rates of success for new drugs and treatment strategies have improved for
patients partipating in the trials cite:[pmid37486997],
cite:[pmid37483175]. Additional studies have also demonstrated that machine
learning algorithms can result in enhanced optimization of clinical trial
designs, reduction in costs, and an overall acceleration of the drug
development pipelines cite:[pmid37479540], cite:[pmid37458097].

==== CONCLUSION

The rapid advancements in the fields of Machine Learning and Artificial
Intelligence in recent years have exerted a substantial influence in the field
of Bioinformatics. With these accelerated developements, the chance to
systematically categorize algorithms and their corresponding applications,
along with their perfomance across various types of bioinformatics data, has
diminished. By harnessing the mathematical formalisms of symmetry and group
theory, we can establish the operational principles of Artificial Intelligence
algorithms concerning bioinformatics data. This not only paves the way for a
deeper understanding of their functionality but also provides insights into the
directions for future development in the field.

*Funding Information:* This work has been supported by Award Number U54
CA221704(5) from The National Cancer Institute.

*Author Contributions:* K. Krampis wrote the manuscript and performed the
research. C. Wultsch provided overview during the development of the research
and the manuscript. E. Ross, O. Ogunwobi, G. Ma and R. Mazumder contributed to
the development of the research and provided feedback during the development of
the manuscript.

*Conflict of Interest:* The authors declare no conflicts of interest.

*Institutional Review Board Statement:* Not Applicable.

*Informed Consent Statement:* Not Applicable.

*Data Availability Statement:* No data were generated as part of the present
review paper.

*Acknowledgments:* The authors would like to thank their respective
institutions for supporting their scholarly work.

*Conflicts of Interest:* The authors declare no conflict of interest.

[1] K. Katz, O. Shutov, R. Lapoint, M. Kimelman, J. R. Brister, and C.
    O’Sullivan, “The sequence read archive: a decade more of explosive growth,”
_Nucleic acids research_, vol. 50, no. D1, pp. D387–D390, 2022.

[2] L. Clissa, “Survey of Big Data sizes in 2021.” 2022.

[3] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, “Geometric deep
    learning: Grids, groups, graphs, geodesics, and gauges,” _arXiv preprint
arXiv:2104.13478_, 2021.

[4] E. Noether, “Invariante variationsprobleme, math-phys,” _Klasse,
    pp235-257_, 1918.

[5] Y. Li, C. Huang, L. Ding, Z. Li, Y. Pan, and X. Gao, “Deep learning in
    bioinformatics: Introduction, application, and perspective in the big data
era,” _Methods_, vol. 166, pp. 4–21, 2019.

[6] T. M. Nair, “Building and Interpreting Artificial Neural Network Models for
    Biological Systems.,” _Methods in molecular biology (Clifton, N.J.)_, vol.
2190, pp. 185–194, 2021, doi: 10.1007/978-1-0716-0826-5_8.

[7] M. Uzair and N. Jamil, “Effects of hidden layers on the efficiency of
    neural networks,” in _2020 IEEE 23rd international multitopic conference
(INMIC)_, 2020, pp. 1–6.

[8] V. Renganathan, “Overview of artificial neural network models in the
    biomedical domain.,” _Bratislavske lekarske listy_, vol. 120, no. 7, pp.
536–540, 2019, doi: 10.4149/BLL_2019_087.

[9] M. Wainberg, D. Merico, A. Delong, and B. J. Frey, “Deep learning in
    biomedicine,” _Nature biotechnology_, vol. 36, no. 9, pp. 829–838, 2018.

[10] B. Tang, Z. Pan, K. Yin, and A. Khateeb, “Recent advances of deep learning
     in bioinformatics and computational biology,” _Frontiers in genetics_,
vol. 10, p. 214, 2019.

[11] J. Zou, Y. Han, and S.-S. So, “Overview of artificial neural networks.,”
     _Methods in molecular biology (Clifton, N.J.)_, vol. 458, pp. 15–23, 2008,
doi: 10.1007/978-1-60327-101-1_2.

[12] N. Kriegeskorte and T. Golan, “Neural network models and deep learning,”
     _Current Biology_, vol. 29, no. 7, pp. R231–R236, 2019.

[13] S. Ruder, “An overview of gradient descent optimization algorithms,”
     _arXiv preprint arXiv:1609.04747_, 2016.

[14] G. Chartrand _et al._, “Deep Learning: A Primer for Radiologists.,”
     _Radiographics : a review publication of the Radiological Society of North
America, Inc_, vol. 37, no. 7, pp. 2113–2131, 2017, doi: 10.1148/rg.2017170077.

[15] N. Kriegeskorte and T. Golan, “Neural network models and deep learning.,”
     _Current biology : CB_, vol. 29, no. 7, pp. R231–R236, Apr.  2019, doi:
10.1016/j.cub.2019.02.034.

[16] S. Agatonovic-Kustrin and R. Beresford, “Basic concepts of artificial
     neural network (ANN) modeling and its application in pharmaceutical
research.,” _Journal of pharmaceutical and biomedical analysis_, vol. 22, no.
5, pp. 717–727, Jun. 2000, doi: 10.1016/s0731-7085(99)00272-1.

[17] A. Eetemadi and I. Tagkopoulos, “Genetic Neural Networks: an artificial
     neural network architecture for capturing gene expression relationships.,”
_Bioinformatics (Oxford, England)_, vol. 35, no. 13, pp. 2226–2234, Jul. 2019,
doi: 10.1093/bioinformatics/bty945.

[18] L. G. Wright _et al._, “Deep physical neural networks trained with
     backpropagation.,” _Nature_, vol. 601, no. 7894, pp. 549–555, Jan. 2022,
doi: 10.1038/s41586-021-04223-6.

[19] W. A. Sutherland, _Introduction to metric and topological spaces_.  Oxford
     University Press, 2009.

[20] M. Lee, “Recent Advances in Deep Learning for Protein-Protein Interaction
     Analysis: A Comprehensive Review,” _Molecules_, vol. 28, no.  13, Jul.
2023.

[21] M. Wysocka, O. Wysocki, M. Zufferey, D. Landers, and A. Freitas, “A
     systematic review of biologically-informed deep learning models for
cancer: fundamental trends for encoding and interpreting oncology data,” _BMC
Bioinformatics_, vol. 24, no. 1, p. 198, May 2023.

[22] B. Jahanyar, H. Tabatabaee, and A. Rowhanimanesh, “Harnessing Deep
     Learning for Omics in an Era of COVID-19,” _OMICS_, vol. 27, no. 4, pp.
141–152, Apr. 2023.

[23] F. W. Pun, I. V. Ozerov, and A. Zhavoronkov, “AI-powered therapeutic
     target discovery,” _Trends Pharmacol Sci_, Jul. 2023.

[24] G. Floresta, C. Zagni, V. Patamia, and A. Rescifina, “How can artificial
     intelligence be utilized for de novo drug design against COVID-19
(SARS-CoV-2)?,” _Expert Opin Drug Discov_, pp. 1–4, Jul. 2023.

[25] Y. Zhou _et al._, “Deep learning in preclinical antibody drug discovery
     and development,” _Methods_, Jul. 2023.

[26] A. rez-Mena, E. n, M. J. Alvarez-Cubero, A. Anguita-Ruiz, L. J.
     Martinez-Gonzalez, and J. Alcala-Fdez, “Explainable artificial
intelligence to predict and identify prostate cancer tissue by gene
expression,” _Comput Methods Programs Biomed_, vol. 240, p. 107719, Jul.  2023.

[27] W. Wei, Y. Li, and T. Huang, “Using Machine Learning Methods to Study
     Colorectal Cancer Tumor Micro-Environment and Its Biomarkers,” _Int J Mol
Sci_, vol. 24, no. 13, Jul. 2023.

[28] D. Shigemizu _et al._, “Classification and deep-learning-based prediction
     of Alzheimer disease subtypes by using genomic data,” _Transl Psychiatry_,
vol. 13, no. 1, p. 232, Jun. 2023.

[29] Z. Mirza _et al._, “Identification of Novel Diagnostic and Prognostic Gene
     Signature Biomarkers for Breast Cancer Using Artificial Intelligence and
Machine Learning Assisted Transcriptomics Analysis,” _Cancers (Basel)_, vol.
15, no. 12, Jun. 2023.

[30] R. Adam, K. Dell’Aquila, L. Hodges, T. Maldjian, and T. Q. Duong, “Deep
     learning applications to breast cancer detection by magnetic resonance
imaging: a literature review,” _Breast Cancer Res_, vol. 25, no. 1, p. 87, Jul.
2023.

[31] Y. Tong _et al._, “Prediction of lymphoma response to CAR T cells by deep
     learning-based image analysis,” _PLoS One_, vol. 18, no. 7, p.  e0282573,
2023.

[32] L. R. Archila _et al._, “Performance of an Artificial Intelligence Model
     for Recognition and Quantitation of Histologic Features of Eosinophilic
Esophagitis on Biopsy Samples,” _Mod Pathol_, p. 100285, Jul. 2023.

[33] Q. Li, A. Sandoval, and B. Chen, “Advancing spinal cord injury research
     with optical clearing, light sheet microscopy, and artificial
intelligence-based image analysis,” _Neural Regen Res_, vol. 18, no. 12, pp.
2661–2662, Dec. 2023.

[34] M. Santorsola and F. Lescai, “The promise of explainable deep learning for
     omics data analysis: Adding new discovery tools to AI,” _N Biotechnol_,
vol. 77, pp. 1–11, Jun. 2023.

[35] B. Waissengrin _et al._, “Artificial intelligence (AI) molecular analysis
     tool assists in rapid treatment decision in lung cancer: a case report,”
_J Clin Pathol_, Jul. 2023.

[36] F. Hosseini, F. Asadi, H. Emami, and M. Ebnali, “Machine learning
     applications for early detection of esophageal cancer: a systematic
review,” _BMC Med Inform Decis Mak_, vol. 23, no. 1, p. 124, Jul. 2023.

[37] S. M. Ahmed, R. V. Shivnaraine, and J. C. Wu, “FDA Modernization Act 2.0
     Paves the Way to Computational Biology and Clinical Trials in a Dish,”
_Circulation_, vol. 148, no. 4, pp. 309–311, Jul. 2023.

[38] A. Aliper _et al._, “Prediction of clinical trials outcomes based on
     target choice and clinical trial design with multi-modal artificial
intelligence,” _Clin Pharmacol Ther_, Jul. 2023.
