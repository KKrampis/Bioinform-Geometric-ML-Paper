== Principles of Artificial Neural Networks and Machine Learning for Bioinformatics Applications

Konstantinos Krampis*^1^, Eric Ross^2^, Olorunseun O. Ogunwobi^1^, Grace Ma,^3^ Raja Mazumder,^4^ Claudia Wultsch^1^


:stem: asciimath
:stem: latexmath 
:bibtex-file: /Users/bioitx/Downloads/Biotechniques/paperbooks-main/GDL-proto.bib

^1^Belfer Research Facility, Biological Sciences, Hunter College, City University of New York, NY, USA
^2^Fox Chase Cancer Center, Philadephia, PA, USA
^3^Center for Asian Health, Lewis Katz School of Medicine, Temple University, Philadelphia, PA, USA
^4^Biochemistry and Molecular Biology, George Washington University, Washington D.C., USA

^*^Corresponding Author, _kk104@hunter.cuny.edu_

*Author Contributions:* K. Krampis wrote the manuscript and performed the
research. C. Wultsch provided overview during the development of the research
and the manuscript. E. Ross, O. Ogunwobi, G. Ma and R. Mazumder contributed to
the development of the research and provided feedback during the development of
the manuscript.

*Conflict of Interest:* The authors declare no conflicts of interest.

=== ABSTRACT
With the exponential growth of machine learning and development of Artificial
Neural Network (ANNs) in recent years, there is great opportunity to leverage
this approach and accelarate biological discoveries through applications in the
analysis of high-throughput data. Various types of datasets, including protein
or gene interaction networks, molecular structures, and cellular signalling
pathways, have already been utilized for machine learning by training ANNs for
inference and pattern classification. However, unlike regular data structures
commonly used in the fields of computer science and engineering, bioinformatics
datasets present challenges that require unique algorithmic approaches. The
recent development of geometric and deep learning approaches within the machine
learning field holds great promise for accelerating the analysis of complex
bioinformatics datasets. Here, we demonstrate the principles of ANNs and their
significance for bioinformatics machine learning by presenting the underlying
mathematical and statistical foundations from group theory, symmetry, and
linear algebra. Furthermore, the structure and functions of ANN algorithms,
which constitute the core principles of artificial intelligence, are explained
in relation to the bioinformatics data domain. In summary, this manuscript
provides guidance for researchers to understand the principles necessary for
practicing machine learning and artificial intelligence, with special
considerations for bioinformatics applications.

*Keywords:*_machine learning, artificial intelligence, bioinformatics, cancer biology, neural networks, symmetry, group theory, algorithms_
biology, neural networks, symmetry, group theory, algorithms_


=== SIMPLE SUMMARY
Here, we provide an overview of the foundational formalisms of Artificial
Neural Networks (ANNs), which serve as the basis for Artificial Intelligence
within the broader field of of Machine Learning.  The review is from the
perspective of bioinformatics data, and multiple examples showcasing the
applications of these formalisms to experimental scenarios are presented
herein. The mathematical formalisms are explained in detail, offering
biologists who are not Machine Learning experts the opportunity to understand
the algorithmic basis of Artificial Intelligence as it relates to
bioinformatics applications.

=== INTRODUCTION
In summary, Artificial Intelligence (AI), Machine Learning (ML), and Deep
Learning (DL) are interconnected concepts with distinct differences: AI is
centered around developing machines capable of performing tasks that require
human intelligence, ML empowers computers to learn from data and make
predictions without explicit programming, and DL employs deep neural networks
to discern patterns from complex datasets. AI encompasses both ML and DL, which
function as subsets of AI. ML algorithms learn patterns from data to facilitate
accurate predictions or decisions and can be categorized into supervised,
unsupervised, and reinforcement learning. DL algorithms, drawing inspiration
from the human brain, utilize deep neural networks to learn and extract
patterns from large-scale datasets. DL has shown success in domains such as
image and speech recognition, natural language processing (NLP), and autonomous
driving.

In the last decade, technologies such as genomic sequencing have led to an
exponential increase cite:[katz2022sequence] in the data describing the
molecular elements, structure, and function of biological systems.
Additionally, data digitization and generation across varied fields such as
physics, software development, and social media cite:[clissa2022survey],
has yielded complex datasets of scales previously unavailable to scientists. AI
also provides many opportunities for healthcare, ranging from clinical
decision-support systems to deep-learning based health information management
systems. This abundance of data has played a pivotal role in the rapid progress of
machine learning, deep learning, and artificial intelligence. As a result, we
now have algorithms that can be trained to extract insights from data with a
level of sophistication that closely resembles human intuition.

While researchers have developed hundreds of successful algorithms, there are
currently a few overarching principles to systematically organize machine
learning algorithms. In a seminal `proto-book` by Bronstein et al.
cite:[bronstein2021geometric], various systematization principles for different
Artificial Neural Network (ANN) architectures and deep learning algorithms were
presented. These principles are founded on the concepts of symmetry and
mathematical group theory. Symmetry and invariance are central concepts in
physics, mathematics, and biological systems. Since the early 20^th^ century,
it has been established that fundamental principles of nature are rooted in
symmetry cite:[noether1918invariante]. The authors also introduced the concept
of geometric deep learning and demonstrated how group theory, along with
function invariance and equivariance principles, can serve as foundation for
composing and describing different deep learning algorithms. Along these lines,
the present manuscript explains the structure of ANNs and the core principles
of machine learning algorithms. Additionally, it offers a review of the
mathematical and statistical foundations pertinent to the development of
artificial intelligence applications using bioinformatics data.

=== THE STRUCTURE OF ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS
We will begin by describing the structures and functions of deep learning and
Artificial Neural Networks (ANNs,*Fig.1*), which form the foundation of artificial
intelligence cite:[li2019deep]. We use a dataset consisting of _n_ pairs of
latexmath:[$\left( x_{i},y_{i} \right)_{n}$], where latexmath:[$x_{i}$]
represents _n_ data points and latexmath:[$y_{i}$] their corresponding labels.
Each latexmath:[$x_{i}$] data point can take the form of a number, a vector (an
array of numbers), or a matrix (a grid of numbers), storing various types of
bioinformatics data. The labels can assume different formats, such as binary
(two-options), like latexmath:[$y_{i} = 1$] "inhibits cancer growth", or
latexmath:[$y_{i} = 0$] "does not inhibit cancer". The labels can also be
continuous numbers, for instance, latexmath:[$y_{i} = 0.3$] indicating 30%
inhibition, or a composite label such as latexmath:[$y_{i} = \left( 0,1,0
\right),$] which signifies drug attributes like '0 - no inhibition', '1 - yes
for toxicity', '0 - not metabolized', respectively. Similarly, the input data
points can also be composite, for example, latexmath:[$x_{i} = \left( 50,100
\right)$] representing two measuments for a single biological entity.

The primary objective deep learning is to train an Artificial
Neural Network (ANN) using this labeled data. This training phase is the
foundation of deep learning algorithms, where the model learns to identify
patterns and relationships between the data points (latexmath:[$x_{i}$]) and
their corresponding labels (latexmath:[$y_{i}$]). This learning process allows
the model to adjust its internal parameters, often referred to as weights and
biases, to minimize the difference between its predicted labels and the actual
labels. Once the model is adequately trained, it can then be used to predict the
labels of new, unseen data points. This capability to generalize from learned
patterns to new data is a critical aspect of deep learning.

Bioinformatics deals with large and complex biological datasets, often
high-dimensional and non-linear. The ability of deep learning algorithms to
handle such complexity makes them particularly suitable for bioinformatics
applications. For instance, a trained ANN can be used to predict the function of
a newly sequenced gene, the potential toxicity of a new drug compound, or the
likelihood of a patient responding to a specific treatment. Therefore, the
overarching goal of artificial intelligence applications in bioinformatics is
not just to classify or predict labels for new data, but to derive meaningful
biological insights from vast and complex data, thus aiding in the advancement
of biological and medical research cite:[Nair2021]."

The structure of an artificial neural network (ANN) can be visualized as a
system of interconnected layers, each comprised of multiple nodes or 'neurons'.
The simplest form of such a network is known as a "fully connected" or "dense"
network, as illustrated in *Fig.1*. In this configuration, each neuron within the
network is connected to every neuron in the adjacent layers, resulting in a
dense web of interactions cite:[Nair2021].

Each neuron in the network, denoted as k, has a specific number of incoming and
outgoing connections. These connections, often referred to as 'synapses',
correspond to the neurons present in the preceding and successive layers within
the network. Each connection carries a 'weight', which is a numerical value that
the network adjusts during the learning process to improve its predictions.

Taking the example of the neuron latexmath:[$k_{1}^{(1)}$] in the First Layer
(1) depicted in *Fig.1*, it has latexmath:[$n = 2$] incoming connections and
latexmath:[$n = 3$] outgoing connections. The incoming connections are from the
"input layer", which consists of two neurons. These neurons represent the input
data points that are fed into the network. The outgoing connections, on the
other hand, extend to the neurons of the subsequent layer, often referred to as
the "hidden layer".

The term "hidden" is used to describe the layers that are sandwiched between the
input and output layers. They are termed so because they do not directly
interact with the external environment, i.e., they neither receive the input
data nor produce the final output. Instead, they perform complex transformations
on the data received from the input layer, passing on the transformed data to
the next layer. This hidden layer, denoted as Second Layer (2) in the figure, is
made up of three neurons, each receiving a connection from the neuron
latexmath:[$k_{1}^{(1)}$].  In summary, the structure of an ANN is a complex,
interconnected system of neurons, each receiving data from the previous layer,
processing it, and passing it on to the next. This intricate structure allows
the network to learn complex patterns in the data, making it a powerful tool for
tasks such as classification, regression, and clustering.

The architecture of an artificial neural network (ANN), particularly the number
of neurons in the hidden layers, is closely tied to the complexity of the
classification problem it aims to solve. This concept is somewhat analogous to
the functioning of neurons in animal brains, where different types of neurons
play different roles in processing sensory information and cognitive tasks.
In an ANN, the hidden layers are the workhorses of the network, responsible for
transforming the input data into a format that can be used for the final
classification or prediction task. The number of neurons in these hidden layers
can vary widely and is often determined based on the complexity of the label
classification problem that the ANN is designed to tackle
cite:[uzair2020effects].

For example, if the ANN is being used to classify simple binary data (such as
whether a given email is spam or not), a single hidden layer with a small number
of neurons might be sufficient. However, for more complex problems (like image
recognition or natural language processing), multiple hidden layers with a
larger number of neurons might be required. This is because complex tasks often
involve identifying higher-level features or patterns in the data, which
requires the network to perform more transformations on the input data.
The input layer of the ANN, on the other hand, must have a specific number of
neurons that align with the structure of the input data. For instance, in Fig.
1, there are two input neurons, suggesting that each input data point is a
two-dimensional vector, like latexmath:[$x_{i} = \left( 50,100 \right)$].
Finally, the output layer of the ANN consists of a number of neurons that
corresponds to the count of labels associated with each input data point in the
dataset. In Fig. 1, there is a single neuron in the output layer, indicating
that each input data point is associated with a single label.


[.middle]
[graphviz, target=Fig1, format=svg]
....
digraph G {

    // ========================
    // rankdir: directed graph drawn from left to right 
    // details: http://www.graphviz.org/doc/info/attrs.html
    // ========================
    rankdir=LR;  
    edge[style=solid, tailport=e];
    nodesep=0.4;
    
    // ========================
    // splines=line:  draw straight lines to connect nodes
    // ========================
    splines=line;
    node [label=""];
    subgraph cluster_0 {
        label="Input layer";
        node [color=chartreuse, 
              style=filled, 
              shape=circle];
        x0 [fillcolor=chartreuse, 
            label=<X<sub>1</sub>>];
        x1 [fillcolor=chartreuse, 
            label=<X<sub>2</sub>>];
 
    }

    subgraph cluster_1 {
        color=white;
        label="First Layer (1)";
        node [color=dodgerblue, 
              style=filled, 
              shape=circle];
        a02 [fillcolor=dodgerblue, 
             label=<k<sub>1</sub><sup>(1)</sup>>];
        a12 [fillcolor=dodgerblue, 
             label=<k<sub>2</sub><sup>(1)</sup>>];
        a22 [fillcolor=dodgerblue, 
             label=<k<sub>3</sub><sup>(1)</sup>>];
    }

    subgraph cluster_2 {
        color=white;
        label="Second Layer (2)";
        node [color=dodgerblue, 
              style=filled, 
              shape=circle];
        a03 [fillcolor=dodgerblue, 
             label=<k<sub>3</sub><sup>(2)</sup>>];
        a13 [fillcolor=dodgerblue, 
             label=<k<sub>2</sub><sup>(2)</sup>>];
        a23 [fillcolor=dodgerblue, 
             label=<k<sub>1</sub><sup>(2)</sup>>];

    }

    subgraph cluster_3 {
 
        label="Output Layer";
        node [color=coral1, 
              style=filled, 
              shape=circle];
        O1 [fillcolor=coral1, 
            label=<Y<sub> </sub>>];
      

    }

   // ========================
   // This is the trick to enforce the bias node stays at the top of 
   // vertical array of nodes in each layer
   // style=invisible: makes the edge connection invisible
   // dir=none: hide the arrow 
   // ========================
    x0 -> a02 [penwidth=0.5];
    x0 -> a12 [penwidth=0.5];
    x0 -> a22 [penwidth=0.5];
    
    x1 -> a02 [penwidth=0.5];
    x1 -> a12 [penwidth=0.5];
    x1 -> a22 [penwidth=0.5];

    a02 -> a03 [penwidth=0.5];
    a02 -> a13 [label=<W<SUB>k1</SUB> * X<SUB>k1</SUB>>, fontcolor=blue, color=red, fontsize=10, penwidth=2.5];
    a02 -> a23 [penwidth=0.5];
 
    a12 -> a03 [penwidth=0.5];
    a12 -> a13 [label=<W<SUB>k2</SUB> * X<SUB>k2</SUB>>,fontcolor=blue, color=red, fontsize=10, penwidth=2.5 ];
    a12 -> a23 [penwidth=0.5];

    a22 -> a03 [penwidth=0.5];
    a22 -> a13 [label=<W<SUB>k3</SUB> * X<SUB>k3</SUB>>,fontcolor=blue, color=red, fontsize=10, penwidth=2.5];
    a22 -> a23 [penwidth=0.5];
 
    a03 -> O1 [penwidth=0.5];
    a13 -> O1 [penwidth=0.5];
    a23 -> O1 [penwidth=0.5];
}
....


''' 
*Figure 1.* An example *Artificial Neural Network (ANN)*. The signal
aggregation taking place on the second neuron
latexmath:[$\sigma_{k_{2}^{(2)}}$] of the second hidden layer, can be expressed
with the formula latexmath:[$\sigma_{k_{2}^{(2)}} =
\sum_{k_{1,2,3}}^{(\begin{matrix} 1 \\ \end{matrix})}w_{k1}*x_{k1} +
w_{k2}*x_{k2} + w_{k3}*x_{k3} - b$], which is the aggregation of neuron signals
from the first layer, shown as red arrows in the figure. _b_ represents the
threshold that needs to be overcome by the aggregation sum in order for the
neuron to fire, and then the neuron will transmit a signal along the line shown
towards the output on the final layer of the figure. The reader should refer to
the text for more details.  
'''

Similar to neural networks in animal brains, the computational abstractions
used in machine learning and artificial intelligence model neurons as
computational units that execute signal summation and threshold activation
cite:[Renganathan2019]. Specifically, each artificial neuron performs a
summation of incoming signals from its connected neighbooring neurons in the
preceding layer on the network, shown for example as red arrows on *Fig.1* for
latexmath:[$\sigma_{k_{2}^{(2)}}$] . The signal processing throughout the ANN
transitions from the input data latexmath:[$x_{i}$] on the leftmost layer
(*Fig.1*) to the output of data labels latexmath:[$y_{i}$] on the rightmost
end.  Within each neuron, when the aggregated input reaches a certain
threshold, the neuron "fires" and transmits a signal to the subsequent layer.

The signals entering the neuron can either be the data directly from the input
layer or signals generated by the activation of neurons in the intermediate
"hidden" layers. The summation and thresholding computation within each neuron
is represented with the function latexmath:[$\sigma_{k} =
\sum_{1}^{k}w_{k}*x_{k} - b$], where latexmath:[$w_{k}$] represents the
connection weights of the preceding neurons.  Each connection arrow in *Fig.1*
has a distinct weight, such as, for example, latexmath:[$x_{k1}$] which is the
incoming signal from the neuron latexmath:[$\sigma_{k_{1}^{(1)}}$]  to neuron
latexmath:[$\sigma_{k_{2}^{(2)}}$] , multiplied by the weight
latexmath:[$w_{k1}$], which symbolizes the strength of the connection between
these two artificial neurons.

The weights in artificial neural networks embody the strength of connections
between neurons. They determine the impact of input signals on the final output
of the network. Throughout the training process, these weights are adjusted to
minimize the difference between the network’s predicted and intended output.
The weights govern the information flow within the network, enabling it to
learn and generate precise predictions. Accurately calibrated weights are
crucial for the network to effectively learn patterns and extrapolate its
knowledge to novel input data cite:[Renganathan2019].

For the majority of applications, the weight values latexmath:[$w_{k}$]
constitute the only elements in the ANN structure that are variable and
adjusted by the algorithms during training using the input data. This process
is similar to the biological brain, where learning takes place by strengthening
connections among neurons cite:[wainberg2018deep].  However, unlike the
biological brain, the ANNs used for practical data analysis have fixed
connections between neurons and the structure of the neural network remains
unaltered during the process of training and learning to recognize and classify
new data. The last term _b_ in the summation signifies a threshold that must be
surpassed, as in latexmath:[$\sum_{1}^{k}w_{k}*x_{k} > b$], to trigger the
activation of a neuron. 

A final step prior to transmitting the neuron’s output value involves the
application of a "logit" function to the summation value that is represented as
latexmath:[$\varphi\left( \sigma_{k} \right)$].  latexmath:[$\varphi$] can be
selected from a range of non-linear functions contingent on the type of input
data and the specific analysis and data classification domain for which the ANN
will be used cite:[li2019deep]. The value of the logit function is the output
of the neuron, which is transmitted to its interconnected neurons in the
subsequent layer through outgoing connections, illustrated as an arrow in
*Fig.1* and corresponding to the brain cell axons in the biological analogy.
Multiple layers of interconnected neurons (*Fig.1*), along with multiple
connections per layer, each having its own weight latexmath:[$w_{k}$], together
form the framework of the Artificial Neural Network (ANN).

From a mathematical formalism perspective, a trained ANN is a function
latexmath:[$f$] that predicts labels latexmath:[$y_{\text{pre}d_{i}}$], which
can include categories such as 'no inhibition', 'yes for toxicity' etc., for
different types of input data latexmath:[$x_{i}$], ranging from histology
images to drug molecules represented as graph data structures. Therefore, the
ANN undertakes data classification by operating as a mapping function
latexmath:[$f\left( x_{i} \right) = y_{\text{pre}d_{i}}$], that connects the
input data to the respective labels. Furthermore, the latexmath:[$f\left( x_{i}
\right)$] is a non-linear function, since it is an aggregate composition of the
non-linear functions latexmath:[$\varphi\left( \sigma_{k} \right)$] of the
individual interconnected neurons within the network cite:[li2019deep]. As
a result, the latexmath:[$f\left( x_{i} \right)$] can successfully classify
labels for data inputs originating from complex data distributions. This fact
enables ANNs to attain heightened analytical capability compared to
conventional statistical learning algorithms cite:[tang2019recent]. The
latexmath:[$f\left( x_{i} \right)$] estimation is carried out by fitting a training
dataset, which establishes correlations between labels latexmath:[$y_{i}$] and
data points latexmath:[$x_{i}$]. With hundreds of papers and monographs that
were written on the technical details of training ANNs, we will next attempt to
briefly summarize the process and direct the reader to provided citations for
further details cite:[Zou2008a].

As mentioned earlier, the only variable elements in the ANN structure are the
weights latexmath:[$w_{k}$] of neuron connections. Therefore, training an ANN
to classify data involves the estimation of these weights. Furthermore, the
training process entails minimizing the error latexmath:[$E$], which is the
difference between the labels latexmath:[$y_{\text{pre}d_{i}}$] predicted by
the function latexmath:[$f$] and the true labels latexmath:[$y_{i}$]. This
error metric is akin to true/false positive and negatives (precision and
recall) used in statistics, however, different formulas are used for its
estimation when dealing with multi-label or complex input data for the ANN (for
further details, refer to cite:[kriegeskorte2019neural]).  The estimation of
neuron connection weights latexmath:[$w_{k}$] is executed by the algorithm
through fitting the network function latexmath:[$f$] to a large training
dataset of latexmath:[$\left\{ x_{i},y_{i} \right\}_{i}^{n}$] pairs of input
data and labels, while the error latexmath:[$E$] is calculated by using a
subset of the data for testing and validation purposes. The training algorithm
starts with an initial value of the weights, and then performs multiple cycles,
referred to as "epochs", to estimate the function latexmath:[$f.$] This is
achieved by fitting the data latexmath:[$x_{i}$] to the network and calculating
the error latexmath:[$E$] by comparison between the predicted
latexmath:[$y_{\text{pre}d_{i}}$] and the true labels latexmath:[$y_{i}$]. At
the end of each cycle, a process called "backpropagation" is performed
cite:[tang2019recent], which involves a gradient descent optimization
algorithm, which fine-tunes the weights of individual neurons to minimize
latexmath:[$E$]. 

The gradient descent cite:[ruder2016overview] optimization examines a large
subset of all possible combinations of weight values, yet as a heuristic
algorithm, it minimizes latexmath:[$E$], but cannot reach zero error. Upon the
completion of multiple training cycles, the training algorithm identifies a set
of weights that best fit the data with minimal error. The ANN settles on the
optimal values that estimate each latexmath:[$\varphi\left( \sigma_{k} \right)$]
function for latexmath:[$\sigma_{k} = \sum_{1}^{k}w_{k}*x_{k} - b$], where
latexmath:[$w_{k}$] is the weight in each interconnected neuron.  Consequently,
the overall function latexmath:[$f$] represented by the network is also
estimated, as it comprises the composition of the individual
latexmath:[$\varphi\left( \sigma_{k} \right)$] neuron functions, as mentioned
earlier. Following the completion of the artificial neural network training,
where the most optimal set of weights is determined, the network is ready to be
used for label prediction with new, unknown latexmath:[$x_{i}$] data.

=== ARTIFICIAL INTELLIGENCE, GROUP THEORY, SYMMETRY AND INVARIANCE

==== Data domains in relation to group theory and symmetry

Graph deep learning is a branch of machine learning that uses graph theory and
deep learning techniques to analyze and interpret data structured as graphs
cite:[bronstein2021geometric].  Graphs are mathematical structures that
represent pairwise relationships between objects. They are composed of vertices
(or nodes) and edges, where vertices represent entities and edges represent
relationships between entities.  In the remaining sections, we will examine how
the principles of group theory, symmetry, and invariance provide a foundational
framework for comprehending the function of machine learning algorwthms.
Furthermore, the classifying power of ANNs, particularly in relation to
statistical variance, transformations, and non-homogeneity in the input data. In
broad terms, symmetry entails the analysis of geometric and algebraic
mathematical structures and finds applications across different research fields,
including physics, molecular biology, and machine learning. A core concept in
symmetry is invariance, which, in our context, is changing data coordinates,
such as relocating a drug molecule in space or shifting the position of a cancer
histology tissue sample, while maintaining the shape of the object unchanged
cite:[wu2020comprehensive]. Following such an alteration, which will be formally
defined later in this text as an _invariant transformation_, it becomes
imperative for the machine learning algorithms and ANNs to be capable of
identifying a drug molecule even after rotation or recognizing cancerous tissue
from a shifted histology image.

Symmetry in the context of graph deep learning refers to the invariance of a
graph under permutations of its nodes. This means that the properties and
characteristics of the graph remain unchanged even if the nodes are rearranged.
This is a crucial aspect to consider when designing graph neural networks (GNNs,
cite:[velickovic2017graph]), the deep learning models used to process graph
data. GNNs need to be invariant or equivariant to different node permutations to
ensure consistent and reliable performance. This is because the same graph can
be represented in many different ways depending on the ordering of the nodes,
and the model should give the same output regardless of this ordering.

Variance, on the other hand, is a measure of how much the values in a dataset
differ from the mean. In the context of graph deep learning, variance can refer
to the diversity in the structure and attributes of the graphs in the dataset.
High variance in the graph data can pose challenges for GNNs, as the model needs
to be able to capture and learn from these variations to make accurate
predictions (cite:[battaglia2018relational], cite:[hamilton2017inductive]).
However, if handled correctly, this variance can also be a powerful source of
information, allowing the model to capture a wide range of patterns and
relationships in the data.

In order to link the abstract symmetry concepts with data classification in
machine learning, as per the terminology of Bronstein et al., we consider the
input data latexmath:[$x_{i}$] to originate from a symmetry domain denoted as
latexmath:[$\Omega$]. This latexmath:[$\Omega$] serves as the foundational
structure upon which the data are based, and it is upon this domain structure
that we train artificial neural networks to undertake classification, employing
the label prediction function latexmath:[$f$] as mentioned in the earlier
section. For example, microscopy images are essentially 2-dimensional numerical
grids of _n x n_ pixels (*Fig.2a*), with each pixel having an assigned value
corresponding to the light intensity captured when the image was taken.  

In this scenario, the data domain is a grid of integers
(latexmath:[$\mathbb{Z}$]), represented as latexmath:[$\Omega:\mathbb{Z}_{n}
\times \mathbb{Z}_{n}$]. Similarly, for color images, the data domain is
latexmath:[$\left. \ x_{i}:\Omega \rightarrow \mathbb{Z}_{n}^{3} \times
\mathbb{Z}_{n}^{3} \right.\ $], encompassing three overlaid integer grids that
individually represent the green, bluem and red layers composing the color
text in the section to explain the concept further and adde 
text in the section to explain the concept further and adde 
all possible combinations of pixel intensities, while the specific pixel value
combinations of the images in the input data latexmath:[$x_{i}$] are a "signal"
latexmath:[$\text{X}\left( \Omega \right)$] from the domain. The ANN’s data
classification and label prediction function latexmath:[$y_{\text{pre}d_{i}} =
f\left( x_{i} \right)$] is applied upon the signal latexmath:[$\text{X}\left(
\Omega \right),$] which fundamentally constitutes a subset of the domain
latexmath:[$\Omega$].

A _symmetry group_ latexmath:[$G$] contains all possible transformations of the
input signal latexmath:[$\text{X}\left( \Omega \right),$] referred to as
symmetries latexmath:[$g$] or _group actions_. A symmetry transformation
latexmath:[$g$] preserves the properties of the data; for instance, it ensures
that objects within an image remain undistorted during rotation. The
constituents of the symmetry group, denoted as latexmath:[$g \in G,$] are the
associations of two or more coordinate points latexmath:[$u,v \in \Omega$] on
the data domain (grid in our image example). Between these coordinates, the
image can undergo rotation, shifting or other transformations without any
distortion.  

Consequently, the key aspect of the formal mathematical definition
of the group lies in its capacity to safeguard data attributes during object
distortions that frequently occur during the experimental acquisition of
bioinformatics data. The concept of symmetry groups is important for modeling
the performance of machine learning algorithms, particularly for classifying
the data patterns despite the variability inherently present within the input
data.

[.left]
[graphviz, target=Fig2a, format=svg]
....
digraph grid_layout {

 node [shape=circle, style=filled, color=lightblue, fontname=Arial, fontsize=11];
 edge [color=gray, penwidth=1.5];
 Pixel1 [label="Pixel 1", color=green];
 Pixel2 [label="Pixel 2", color=blue];
 Pixel3 [label="Pixel 3", color=red];
 Pixel4 [label="Pixel 4", color=yellow];
 Pixel5 [label="Pixel 5", color=orange];
 Pixel6 [label="Pixel 6", color=purple];
 {rank=same; Pixel1; Pixel2; Pixel3;}
 {rank=same; Pixel4; Pixel5; Pixel6;}
 Pixel1 -> Pixel2 -> Pixel3;
 Pixel4 -> Pixel5 -> Pixel6;
 Pixel1 -> Pixel4;
 Pixel2 -> Pixel5;
 Pixel3 -> Pixel6;
}
....


[.right]
[graphviz, target=Fig2b, format=svg]
....
digraph Protein {
    // Define attributes for the graph
    graph [fontsize=16 fontname="Arial" compound=true rankdir="LR"];
    node [shape=circle fontsize=22 fontname="Arial" style=filled];
    edge [fontsize=12 fontname="Arial" penwidth=5.0];

    // Define nodes with attributes and colors
    A [label="Gly\nMol. W: 75.07" fillcolor="blue" fontcolor="white"];
    B [label="Ala\nMol. W: 89.09" fillcolor="red" fontcolor="white"];
    C [label="Val\nMol. W: 117.15" fillcolor="green" fontcolor="white"];
    D [label="Leu\nMol. W: 131.17" fillcolor="yellow" fontcolor="black"];
    E [label="Ile\nMol. W: 131.17" fillcolor="pink" fontcolor="black"];
    F [label="Ser\nMol. W: 105.09" fillcolor="purple" fontcolor="white"];
    G [label="Thr\nMol. W: 119.12" fillcolor="orange" fontcolor="black"];
    H [label="Cys\nMol. W: 121.16" fillcolor="brown" fontcolor="white"];
    I [label="Tyr\nMol. W: 181.19" fillcolor="cyan" fontcolor="black"];
    J [label="Asn\nMol. W: 132.12" fillcolor="magenta" fontcolor="white"];

    // Define edges with attributes
    A -> B;
    B -> C;
    C -> D;
    D -> E;
    E -> F;
    F -> G;
    G -> H;
    H -> I;
    I -> J;
    J -> A;
    A -> D;
    B -> E;
    C -> F;
    D -> G;
    E -> H;
    F -> I;
    G -> J;
    H -> A;
    I -> B;
    J -> C;
}
....


'''
*Figure 2. (a).* A _grid_ data structure representing image pixels, is 
formally a _graph_ *(b).* A _graph_ latexmath:[$G = (V, E)$], is composed of
_nodes_ latexmath:[$V$] shown as circles, and _edges_  connecting the nodes and
shown as arrows. It can represent a protein, where the amino acids are the
nodes and the peptide bonds between amino acids are the edges.

'''



Another important data structure within bioinformatics is a _graph_ denoted as
latexmath:[$G = (V,E)$], composed of _nodes_ latexmath:[$V$] that signify
biological entities, and _edges_ representing connections between pairs of
nodes (*Fig.* *2b*). In a specific instance of a graph corresponding to a
real-world object, the edges are a subset of all possible links between nodes.
An example graph data structure for a biological molecule such a protein or a
drug would portray the amino acids or atoms as node entities, while the
chemical bonds between each of these entities are captured as edges. These
edges could signify the carbonyl-amino (C-N) peptide bonds between amino acids
and molecular interactions across the peptide chain on the protein structure,
or the chemical bonds between atoms in a drug molecule
cite:[Kriegeskorte2019]. 

Furthermore, attributes in the molecular data such as, for example, polarity,
amino acid weight, or drug binding properties can be depicted as
latexmath:[$s$] - dimensional node attributes, where _s_ represents the
attributes assigned to each node.  Similarly, edges or even entire graphs can
have attributes, for experimental data measured on the molecular interactions
represented by the edges, and measurements of the properties of the complete
protein or drug. Finally, from an algorithmic perspective, images can be viewed
as a special case of graphs in which the pixels serve as nodes, interconnected
by edges following a structured pattern that generates a grid formation
(*Fig.2a*) representing the adjacent positions of the pixels.

==== Group theory and symmetry principles applied to machine learning

Having established the mathematical and algorithmic parallels between graphs
and images, we will now utilize the principles of the _symmetry group_
latexmath:[$G$] to examine the analytical and classification power of machine
learning ANNs, with respect to data variability and transformations. Whether it
involves data types like input images or molecules represented as graphs, which
may undergo shifts or rotations, we introduce the concept of invariance guided
by the principles of group theory and symmetry. These foundational mathematical
and algorithmic formalisms serve as the basis for modeling the performance and
output of machine learning algorithms, specifically ANNs, with regard to the
diversity present in the dataset. 

Consecutively, these principles can be extrapolated and generalized to
encompass other types of data beyond graphs and images, for which ANNs are
trained to predict and categorize.  While we present the group and symmetry
definitions following a data-centric approach, we will remain consistent with
the mathematical framework, while describing how the group operations can
effect transformations on the input data. Furthermore, different types of data
may have the same symmetry group, and different transformations could be
performed through identical group operations. For example, an image featuring a
triangle, which essentially is a graph with three nodes, might possess the same
rotational symmetry group as a graph with three nodes or a numerical sequence
of three elements.

When chemical and biological molecules are represented as graphs as described
earlier, the nodes latexmath:[$V$] can be in any order depending on how the
data were measured during the experiment. However, this variation does not
change the underlying information contained in the data. As long as the edges
*E,* which represent the connections between molecules, remain unchanged, we
maintain an accurate representation of the molecular entity, irrespective of
the sequence of nodes in *V*. In cases where two graphs portraying the same
molecule have identical edges but differ in node arrangement, they are called
_isomorphic_. It is crucial that any machine learning algorithm designed for
pattern recognition on graphs, should not depend on the ordering of nodes. This
ensures that classification using ANNs and artificial intelligence remain
robust against variations in experiment measurement encountered in real-world
data cite:[AgatonovicKustrin2000]. This is something that is taken for
granted with human intelligence, where, for example, we can recognize an object
even when a photograph is rotated at an angle. 

==== Invariance and the classification power of artificial neural networks

Returning to our earlier formal definitions of ANNs as function estimators
fitted to the data, in order for ANNs algorithms to equivalently recognize
_isomorphic_ graphs, the functions latexmath:[$\varphi\left( \sigma_{k}
\right)$] and overall latexmath:[$f\left( x_{i} \right)$] of the ANN acting on
graph data should be _permutation invariant_. This implies that for any
permutation of the input dataset, the output values of these functions remain
unchanged, regardless of the ordering of the nodes *V*. This concept can be
similarly applied to images, which, as previously mentioned, are specialized
instances of fully connected graphs.  Furthermore, these principles can also be
generalized for other data types beyond images or graphs.

To further formalize the concept of invariance, and considering that both image
and graph examples are essentially points on a grids on a two-dimemensional
plane, we can use linear algebra. Specifically, by using a matrix we can
represent the data transformations as group actions, denoted by
latexmath:[$g$], within the symmetry group latexmath:[$G$]. The use of matrices
enables us to connect the group symmetries with the actual data by performing
matrix multiplications that modify the coordinates of the object and
consecutively represent the data transformations through the multiplication.
The dimensions of the matrix, latexmath:[$n \times n,$] typically are similar
to these of the signal space latexmath:[$\text{X}\left( \Omega \right)$] for
the data (e.g., latexmath:[$\mathbb{Z}_{n} \times \mathbb{Z}_{n}$] images).
The matrix dimensions not depend on the size of the group (i.e. the number of
possible symmetries) or the dimensionality of the underlying data domain
latexmath:[$\Omega$]. With this definition in place, we can formalize
symmetries and group actions for modifying data objects, employing matrix and
linear transformations as the foundation for connecting invariance in relation
to variability in the data.

We will now conclude by establishing the mathematical and linear algebra
formalisms that underlie the resilience of ANNs and machine learning algorithms
in pattern recognition, considering transformations in the data. While our
framework is based on a two-dimensional grid data domain latexmath:[$\Omega$],
the formalisms developed here can also be extrapolated to any number of
dimensions or data formats without loss of generality. First, we will connect
matrices to group actions latexmath:[$g$] (such as rotations, shifts) within
the symmetry group latexmath:[$g \in G$] by defining a function
latexmath:[$\theta$] that maps the group to a matrix as latexmath:[$\theta:G
\rightarrow \mathbf{M}$]. As mentioned earlier, a matrix latexmath:[$\mathbf{M}
\in R^{n \times n}$] consisting of numerical values (integers, fractions,
positive and negative), when multiplied by the coordinate values of an object
on the plane latexmath:[$\Omega$], results in rotation or shifts of the
object’s coordinates for the exact amount corresponding to the group action
within the symmetry group.

With these definitions in place, we will now connect the matrix formalisms with
the neural network estimator function latexmath:[$y_{\text{pre}d_{i}} = f\left(
x_{i} \right)$], which is identified by adjusting neuron connection weights
during multiple training cycles with the input data. Our goal is to leverage
the mathematical formalisms of group symmetry and invariance to establish the
resilience of ANNs in classifying and assigning labels to new data points
cite:[Eetemadi2019]. These data points originate from real-world data that
might contain tranformations and distortions.  First, we define the estimator
function of the ANN to be _invariant_ if the condition for the input data
holds, i.e.  latexmath:[$f(\mathbf{M} \times x_{i}) = f(x_{i})$] for all
matrices latexmath:[$\mathbf{M}$] representing the actions latexmath:[$g \in
G$] within the symmetry group. 

This formula encapsulates the requirement for the neural network function to be
invariant: its output value remains the same whether the input data
latexmath:[$x_{i}$] are transformed or not (e.g., an image or graph is not
rotated on the plane), as represented by the matrix multiplication
latexmath:[$\mathbf{M} \times x_{i}$]. Therefore, the output values
latexmath:[$y_{\text{pre}d_{i}} = f\left( x_{i} \right)$] produced by the ANN,
which essentially represent predicted output labels (e.g.,
latexmath:[$y_{\text{pre}d_{i}}$] = potent drug / not potent), based on the
input data, exhibit resilience to noisy and deformed real-world data when the
network estimator function is invariant. In a different case, the estimator
function approximated by the ANN can be _equivariant_ and defined as
latexmath:[$f(\mathbf{M} \times x_{i}) = \mathbf{M} \times f(x_{i})$].  This
signifies that the output of the ANN will be modified, but the label prediction
result will shift equally alongside the shift in the input data.

==== Neural networks and group theory in relation to continuous data transformations 

Up to this point, we have exclusively discussed discrete tranformations in
linear algebra terms, utilizing matrix multiplications that lead to coordinate
shifts and rigid transformations of the data, like rotating an image or graph
by a specific angle on the grid latexmath:[$\Omega$].  However, in real-world
data scenarios, we often also encounter continuous, more fine-grained shifts.
In such cases, ANNs algorithms should be able to recognize patterns, classify,
and label the data without any loss of performance cite:[Wright2022].
Mathematically, the continuous transformations follow equally with the
invariant and equivariant functions described earlier. For instance, if the
domain latexmath:[$\Omega$] contains data with smooth transformations and
shifts, such as moving images (videos) or shifts of molecules and graphs that
maintain _continuity_ in a topological definition
cite:[sutherland2009introduction], in this case we deal with a concept
known as _homeomorphism_ instead of _invariance_.

Finally, if the rate of continuous transformation of the data is quantifiable,
meaning that the function latexmath:[$\theta,$] which maps the group to a
matrix, is _differentiable_, then the members of the symmetry groups will be
part of a _diffeomorphism_. As it follows from the principles of calculus, in
this case, infinitely multiple matrices latexmath:[$f(\mathbf{(}M)$] will be
needed to be produced by latexmath:[$\theta$] for the continuous change of the
data coordinates at every point. These differentiable data structures are
common with manifolds, which, for example, could be used to represent proteins
in fine detail. In this case, the molecule would be represented as a cloud with
all atomic forces surrounding the structure, as opposed to the discrete data
structure of nodes and edges in a graph. Finally, if the manifold structure
also includes a metric of _distance_ between its points to further quantify the
data transformations, in this case, we will have an _isometry_ during the
transformation due to a group action from the symmetry group.

=== APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS IN BIOINFORMATICS

Artificial Intelligence (AI) and Deep Learning have emerged as powerful tools
with diverse applications in the field of bioinformatics, and multiple research
studies have been reported in the literature cite:[pmid37446831],
cite:[pmid37189058], cite:[pmid37043378], highlighting the potential of
the technology to revolutionize healthcare and life sciences. One of the
significant applications is drug discovery, as AI algorithms facilitate the
analysis of large datasets of chemical compounds, predicting their
effectiveness and safety cite:[pmid37479540], cite:[pmid37458097],
cite:[pmid37454742]. These studies have demonstrated that AI can accelerate
the drug discovery process by screening potential candidates and optimizing
their properties, resulting in substantial cost and time savings.

In the field of genomics, AI algorithms have been applied to the analysis of
DNA sequencing and gene expression data, facilitating, for example, the
identification of disease-causing mutations and enhancing our understanding of
genetic variations cite:[pmid37453366], cite:[pmid37446311],
cite:[pmid37386009], cite:[pmid37370847].  Moreover, in these studies, genomic
data analysis with AI algorithms has provided critical insights, which can
assist in the development of personalized medicine approaches and as result
tailor treatments to individual patients. Consecutively, the use of AI
algorithms in bioinformatics can contribute to the advancement of precision
medicine.  By integratively analyzing also other omics data (e.g.,
transcriptomics, proteomics, metabolomics), patient data, encompassing genetic
information, medical history, and lifestyle factors, AI-driven insights can
lead to improved predictions of drug responses, identification of potential
side effects, and the recommendation of optimal treatment options for
individual patients.

This personalized medicine approach can also involve enhancing patient care and
treatment outcomes, through disease diagnosis improved by machine learning
analysis of medical images, including computed tomography (CT) and magnetic
resonance imaging (MRI) scans, X-rays, and histopathology images, of diseases
like cancer cite:[pmid37488621], cite:[pmid37478073], cite:[pmid37474003],
cite:[pmid37449611].  The AI algorithms can assist pathologists and
radiologists in rendering precise diagnoses, enabling early detection and
diagnosis, and ultimately contributing to overall improvements in patient
outcomes.

AI can also play a significant role in assisting the development of
bioinformatics tools and software accelerating the process of code development
for the analysis and interpretation of biological data, such as sequence
alignment, protein structure prediction, and functional annotation
cite:[pmid37329982], cite:[pmid37463768], cite:[pmid37460991].  Furthermore,
AI-powered natural language processing techniques have been
employed to analyze scientific literature, patents, and clinical trial reports.
This capability enables researchers to stay updated about the latest
discoveries and facilitates knowledge discovery in the field.

Finally, in the area of clinical trials, machine learning algorithms have been
appplied to mine vast amounts of data from clinical trials. As a result, the
rates of success for new drugs and treatment strategies have improved for
patients partipating in the trials cite:[pmid37486997],
cite:[pmid37483175]. Additional studies have also demonstrated that machine
learning algorithms can result in enhanced optimization of clinical trial
designs, reduction in costs, and an overall acceleration of the drug
development pipelines cite:[pmid37479540], cite:[pmid37458097].

==== CONCLUSION

The rapid advancements in the fields of Machine Learning and Artificial
Intelligence in recent years have exerted a substantial influence in the field
of Bioinformatics. With these accelerated developements, the chance to
systematically categorize algorithms and their corresponding applications,
along with their perfomance across various types of bioinformatics data, has
diminished. By harnessing the mathematical formalisms of symmetry and group
theory, we can establish the operational principles of Artificial Intelligence
algorithms concerning bioinformatics data. This not only paves the way for a
deeper understanding of their functionality but also provides insights into the
directions for future development in the field.

*Funding Information:* This work has been supported by Award Number U54
CA221704(5) from The National Cancer Institute.

*Institutional Review Board Statement:* Not Applicable.

*Informed Consent Statement:* Not Applicable.

*Data Availability Statement:* No data were generated as part of the present
review paper.

*Acknowledgments:* The authors would like to thank their respective
institutions for supporting their scholarly work.

*References*:

bibliography::[]